{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a RAG Application with Ollama deepseek-r1:32b, Llama Index, and LangChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this step-by-step guide on building a **Retrieval-Augmented Generation (RAG)** application! In this notebook, we will combine the power of retrieval methods with advanced language generation techniques. Our goal is to create a system that can retrieve relevant information from your data sources and use a state-of-the-art language model to generate insightful responses.\n",
    "\n",
    "### What is RAG?\n",
    "\n",
    "RAG stands for *Retrieval-Augmented Generation*. It is an approach that:\n",
    "- **Retrieves**: Searches and fetches relevant documents or pieces of data.\n",
    "- **Generates**: Leverages a large language model (LLM) to produce contextually accurate and insightful outputs based on the retrieved information.\n",
    "\n",
    "This technique is especially useful for tasks such as:\n",
    "- Question Answering\n",
    "- Chatbots and Conversational Agents\n",
    "- Document Summarization\n",
    "- Knowledge-based Systems\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Ollama deepseek-r1:32b**:  \n",
    "   A powerful model used for embedding and retrieval. We'll explain how to set it up and use it effectively, even if you are starting from scratch.\n",
    "\n",
    "2. **Llama Index**:  \n",
    "   A tool to efficiently build and manage indexes over your data. It simplifies organizing and querying your documents.\n",
    "\n",
    "3. **LangChain**:  \n",
    "   A versatile framework that helps integrate various components (like LLMs and indexes) into a coherent application. It provides a high-level interface to work with large language models.\n",
    "\n",
    "### What to Expect\n",
    "\n",
    "In the sections that follow, we will cover:\n",
    "- **Setting Up Your Environment**:  \n",
    "  How to install and configure Ollama deepseek-r1:32b, including a dedicated section for those who haven’t set it up yet.\n",
    "\n",
    "- **Data Ingestion & Indexing with Llama Index**:  \n",
    "  Step-by-step instructions on how to prepare your data and build an index to enable efficient retrieval.\n",
    "\n",
    "- **Integrating with LangChain**:  \n",
    "  How to tie everything together by interfacing the index with your language model for retrieval-augmented generation.\n",
    "\n",
    "- **Example Use Cases & Exercises**:  \n",
    "  Practical code snippets and exercises to help you apply what you’ve learned in real-world scenarios.\n",
    "\n",
    "By the end of this notebook, you’ll have a clear understanding of how to build and deploy your own RAG application, empowering you to tackle complex information retrieval and generation tasks.\n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup\n",
    "\n",
    "In this section, we will prepare our environment by installing the necessary Python libraries, setting up Ollama with the **deepseek-r1:32b** model, and verifying that our setup is working correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Installing Required Libraries\n",
    "\n",
    "Our RAG application will leverage the following key Python packages:\n",
    "- **LangChain**: For integrating with large language models.\n",
    "- **Llama Index**: For building and querying document indexes.\n",
    "- **Requests**: For making HTTP calls (useful if you interact with an API).\n",
    "\n",
    "Open your terminal or command prompt and run the following command to install these packages:\n",
    "\n",
    "```bash\n",
    "pip install langchain llama-index requests\n",
    "```\n",
    "\n",
    "For the latest installation instructions or updates, please refer to the official documentation:\n",
    "- [LangChain GitHub Repository](https://github.com/hwchase17/langchain)\n",
    "- [Llama Index Documentation](https://gpt-index.readthedocs.io/en/latest/)\n",
    "- [Requests Documentation](https://requests.readthedocs.io/en/latest/)\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2. Setting Up Ollama and deepseek-r1:32b\n",
    "\n",
    "**Ollama** is a platform that enables you to run large language models locally. In our case, we will use it to host the **deepseek-r1:32b** model.\n",
    "\n",
    "### Steps to Set Up:\n",
    "\n",
    "1. **Install Ollama**:  \n",
    "   Visit the [Ollama website](https://ollama.com) and follow the installation instructions for your operating system.\n",
    "\n",
    "2. **Download deepseek-r1:32b**:  \n",
    "   If you haven’t already downloaded the model, you can pull it via the Ollama CLI:\n",
    "   ```bash\n",
    "   ollama pull deepseek-r1:32b\n",
    "   ```\n",
    "\n",
    "3. **Start the Model**:  \n",
    "   Ensure that the model is running on your machine. The exact steps might vary depending on your installation. Consult the [Ollama documentation](https://ollama.com/docs) for detailed guidance.\n",
    "\n",
    "> **Note:** If you prefer using an HTTP API (if provided by your Ollama installation) over the CLI, instructions will be provided later in the notebook.\n",
    "\n",
    "Once you have completed these steps, your local deepseek-r1:32b model should be ready for use.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 2.3. Verifying the Setup\n",
    "\n",
    "Before proceeding, let’s verify that both our Python environment and deepseek-r1:32b are working as expected.\n",
    "\n",
    "#### 2.3.1. Verify Python Package Installation\n",
    "\n",
    "Run the following code snippet in a Python cell to ensure that all necessary packages are installed and importable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import llama_index\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2. Verify deepseek-r1:32b via the Ollama CLI\n",
    "\n",
    "If you’re using the Ollama CLI to interact with deepseek-r1:32b, you can run a quick test. Create a helper function in your notebook that sends a prompt to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Response: <think>\n",
      "Okay, so I'm trying to figure out how to confirm if the system named deepseek-r1:32b is running. Hmm, first off, I need to understand what exactly this refers to. The name seems like it could be a specific instance or model of an AI or some kind of software. Maybe it's related to machine learning or artificial intelligence processing units?\n",
      "\n",
      "I'm not entirely sure how to approach this. Perhaps I should start by checking if there are any status indicators or logs that show whether the system is active. If I have access to the server where deepseek-r1:32b is running, maybe I can look at its process list using commands like 'top' or 'htop' in Linux. That would help me see if it's currently using CPU or memory resources.\n",
      "\n",
      "Another thought: Maybe there's a web interface or dashboard that monitors system instances. If I can log into such a portal, I might find status information about deepseek-r1:32b there. I should check if I have the credentials for any monitoring tools associated with this system.\n",
      "\n",
      "If I don't have access to those resources, perhaps reaching out to the system administrator or support team would be the next step. They might have better visibility into whether the instance is operational or not. It's important to respect permissions and not try to access systems without proper authorization.\n",
      "\n",
      "I also wonder if there are any error logs that could indicate issues with deepseek-r1:32b. Checking log files in directories like /var/log/ on a Linux system might reveal messages about the system's status. If I find recent entries related to deepseek-r1:32b, they might explain whether it's running or if there were any startup problems.\n",
      "\n",
      "Another possibility is using command-line tools to check the service status. For example, in some systems, you can use 'systemctl' to see if a service is active. If I know the service name, I could run something like 'systemctl status deepseek-r1:32b.service'. That would give me information about whether it's running, stopped, or in another state.\n",
      "\n",
      "Wait, but maybe I don't have direct access to the server. In that case, perhaps there are API endpoints or other remote monitoring tools that can provide this information. If there's an API, I might send a request to check the status of the instance. For example, making a GET request to a specific endpoint might return JSON data indicating if it's active.\n",
      "\n",
      "Also, considering network connectivity, maybe pinging the system or checking its ports could help. Using 'ping' to see if it's reachable and then using 'telnet' or 'nc' to check if certain ports are open might indicate whether the service is up. However, I should be cautious with this approach because not all services respond to pings, and some ports might be blocked by firewalls.\n",
      "\n",
      "If the system uses a containerization platform like Docker, checking the running containers could help. Commands like 'docker ps' would list active containers, and if deepseek-r1:32b is there, it's probably running. Similarly, for Kubernetes clusters, using 'kubectl get pods' might show if the pod corresponding to this instance is in a running state.\n",
      "\n",
      "I should also consider any monitoring tools that track system health, like Prometheus or Grafana. These tools often have dashboards where you can see real-time metrics about various services, including whether they're active and performing as expected.\n",
      "\n",
      "Another angle: If deepseek-r1:32b is part of a larger application, maybe there's an internal status page or a heartbeat mechanism that periodically reports its availability. Accessing such a page could quickly confirm if the system is operational.\n",
      "\n",
      "I'm also thinking about how to interpret the response from the system. If I try running commands and get errors, it might mean the service isn't available. But without clear indicators, it's tricky. Maybe looking for error messages or absence of expected outputs can help deduce its status.\n",
      "\n",
      "In summary, my approach would be:\n",
      "1. Check server processes using tools like 'top' or 'htop'.\n",
      "2. Review system logs in /var/log/ for relevant entries.\n",
      "3. Use service management commands like 'systemctl' to check the service status.\n",
      "4. Access monitoring dashboards or APIs to get real-time information.\n",
      "5. Utilize containerization platforms' CLI tools to inspect running instances.\n",
      "6. Consult with IT support if I don't have sufficient access or knowledge.\n",
      "\n",
      "I need to make sure each step is followed carefully, respecting any permissions and not causing disruption. It's important to methodically check each possible avenue to accurately determine the status of deepseek-r1:32b.\n",
      "</think>\n",
      "\n",
      "To confirm whether the system named `deepseek-r1:32b` is running, follow this organized approach:\n",
      "\n",
      "1. **Check Server Processes**:\n",
      "   - Use commands like `top`, `htop`, or `ps aux | grep deepseek-r1:32b` to see if the process is active.\n",
      "\n",
      "2. **Review System Logs**:\n",
      "   - Look in `/var/log/` for any logs related to `deepseek-r1:32b`. Use commands like `journalctl -u <service_name>` for systemd services.\n",
      "\n",
      "3. **Service Status with systemctl**:\n",
      "   - If the system uses systemd, check the service status with `systemctl status deepseek-r1:32b.service`.\n",
      "\n",
      "4. **Monitoring Dashboards or APIs**:\n",
      "   - Access any monitoring tools like Prometheus/Grafana or use API endpoints to query the service's status.\n",
      "\n",
      "5. **Containerization Platforms**:\n",
      "   - For Docker, run `docker ps` to check for running containers named `deepseek-r1:32b`.\n",
      "   - For Kubernetes, use `kubectl get pods` to see if the pod is active.\n",
      "\n",
      "6. **Network Connectivity Checks**:\n",
      "   - Use `ping`, `telnet`, or `nc` to check if the system is reachable and its ports are open.\n",
      "\n",
      "7. **Internal Status Mechanisms**:\n",
      "   - Check for internal status pages or heartbeat mechanisms that indicate the system's availability.\n",
      "\n",
      "8. **Consult IT Support**:\n",
      "   - If you lack access or knowledge, contact your system administrator or support team.\n",
      "\n",
      "By systematically following these steps, you can determine if `deepseek-r1:32b` is running while respecting permissions and minimizing disruption.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def query_deepseek(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Sends a prompt to the deepseek-r1:32b model via the Ollama CLI.\n",
    "    \"\"\"\n",
    "    command = [\"ollama\", \"run\", \"deepseek-r1:32b\", prompt]\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"Error calling deepseek-r1:32b: {result.stderr}\")\n",
    "    \n",
    "    return result.stdout.strip()\n",
    "\n",
    "# Test the function:\n",
    "try:\n",
    "    test_response = query_deepseek(\"Hello, deepseek-r1:32b! Please confirm you are running.\")\n",
    "    print(\"Model Response:\", test_response)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3. (Optional) Verify deepseek-r1:32b via an HTTP API\n",
    "\n",
    "If your Ollama installation provides an HTTP API endpoint, you can test it using the `requests` library. Adjust the API endpoint as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Response: {'model': 'deepseek-r1:32b', 'created_at': '2025-02-02T07:37:41.133629Z', 'response': \"<think>\\n\\n</think>\\n\\nThe sky appears blue because of a phenomenon called Rayleigh scattering. When sunlight passes through Earth's atmosphere, it interacts with molecules and small particles in the air. Sunlight is made up of different colors, each with its own wavelength. Blue light has a shorter wavelength compared to other colors like red or orange.\\n\\nAs sunlight travels through the atmosphere, the shorter blue wavelengths are scattered in all directions by the gases and molecules in the air, especially nitrogen and oxygen molecules. This scattering is much more effective for blue light than for the longer wavelengths like red. So, when you look up at the sky, you see the scattered blue light coming from all over the sky.\\n\\nThis explains why the sky looks blue during the day. At sunrise or sunset, the light has to pass through more of the atmosphere, which scatters away the blue light, leaving behind the longer wavelengths like red and orange, creating those beautiful colors in the sky during those times.\", 'done': True, 'done_reason': 'stop', 'context': [151644, 10234, 374, 279, 12884, 6303, 30, 151645, 151648, 271, 151649, 198, 198, 785, 12884, 7952, 6303, 1576, 315, 264, 24844, 2598, 13255, 62969, 71816, 13, 3197, 39020, 16211, 1526, 9237, 6, 82, 16566, 11, 432, 83161, 448, 34615, 323, 2613, 18730, 304, 279, 3720, 13, 8059, 4145, 374, 1865, 705, 315, 2155, 7987, 11, 1817, 448, 1181, 1828, 45306, 13, 8697, 3100, 702, 264, 23327, 45306, 7707, 311, 1008, 7987, 1075, 2518, 476, 18575, 13, 198, 198, 2121, 39020, 34192, 1526, 279, 16566, 11, 279, 23327, 6303, 92859, 525, 36967, 304, 678, 17961, 553, 279, 44512, 323, 34615, 304, 279, 3720, 11, 5310, 46403, 323, 23552, 34615, 13, 1096, 71816, 374, 1753, 803, 7373, 369, 6303, 3100, 1091, 369, 279, 5021, 92859, 1075, 2518, 13, 2055, 11, 979, 498, 1401, 705, 518, 279, 12884, 11, 498, 1490, 279, 36967, 6303, 3100, 5001, 504, 678, 916, 279, 12884, 13, 198, 198, 1986, 14758, 3170, 279, 12884, 5868, 6303, 2337, 279, 1899, 13, 2411, 63819, 476, 42984, 11, 279, 3100, 702, 311, 1494, 1526, 803, 315, 279, 16566, 11, 892, 1136, 10175, 3123, 279, 6303, 3100, 11, 9380, 4815, 279, 5021, 92859, 1075, 2518, 323, 18575, 11, 6825, 1846, 6233, 7987, 304, 279, 12884, 2337, 1846, 3039, 13], 'total_duration': 17852897209, 'load_duration': 827613667, 'prompt_eval_count': 9, 'prompt_eval_duration': 2262000000, 'eval_count': 194, 'eval_duration': 14762000000, 'telemetry': {'elapsed_time_seconds': 18.086580375000267, 'status_code': 200, 'response_size_bytes': 2306, 'usage': 'Not provided'}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def query_deepseek_api(prompt: str) -> dict:\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"deepseek-r1:32b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    response = requests.post(url, json=payload)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    result = query_deepseek_api(\"Why is the sky blue?\")\n",
    "    print(\"API Response:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Response: {'model': 'deepseek-r1:32b', 'created_at': '2025-02-02T07:40:00.147784Z', 'response': \"<think>\\n\\n</think>\\n\\nThe sky appears blue because of a phenomenon called Rayleigh scattering. When sunlight reaches Earth's atmosphere, it interacts with molecules and small particles in the air. Sunlight is made up of different colors, each with its own wavelength. Blue light has a shorter wavelength compared to other colors like red or orange.\\n\\nAs sunlight travels through the atmosphere, the shorter blue wavelengths are scattered in all directions by the gases and particles in the air. This scattering is much more effective for blue light than for longer wavelengths like red. So, when you look up at the sky, you're seeing the scattered blue light coming from all over the sky.\\n\\nDuring sunrise or sunset, the sun is lower on the horizon, so its light has to pass through more of the Earth's atmosphere. This causes more scattering of the shorter blue wavelengths, allowing the longer red and orange wavelengths to dominate, which is why sunsets often appear red or orange.\", 'done': True, 'done_reason': 'stop', 'context': [151644, 10234, 374, 279, 12884, 6303, 30, 151645, 151648, 271, 151649, 198, 198, 785, 12884, 7952, 6303, 1576, 315, 264, 24844, 2598, 13255, 62969, 71816, 13, 3197, 39020, 24491, 9237, 6, 82, 16566, 11, 432, 83161, 448, 34615, 323, 2613, 18730, 304, 279, 3720, 13, 8059, 4145, 374, 1865, 705, 315, 2155, 7987, 11, 1817, 448, 1181, 1828, 45306, 13, 8697, 3100, 702, 264, 23327, 45306, 7707, 311, 1008, 7987, 1075, 2518, 476, 18575, 13, 198, 198, 2121, 39020, 34192, 1526, 279, 16566, 11, 279, 23327, 6303, 92859, 525, 36967, 304, 678, 17961, 553, 279, 44512, 323, 18730, 304, 279, 3720, 13, 1096, 71816, 374, 1753, 803, 7373, 369, 6303, 3100, 1091, 369, 5021, 92859, 1075, 2518, 13, 2055, 11, 979, 498, 1401, 705, 518, 279, 12884, 11, 498, 6, 265, 9120, 279, 36967, 6303, 3100, 5001, 504, 678, 916, 279, 12884, 13, 198, 198, 16014, 63819, 476, 42984, 11, 279, 7015, 374, 4722, 389, 279, 34074, 11, 773, 1181, 3100, 702, 311, 1494, 1526, 803, 315, 279, 9237, 6, 82, 16566, 13, 1096, 11137, 803, 71816, 315, 279, 23327, 6303, 92859, 11, 10693, 279, 5021, 2518, 323, 18575, 92859, 311, 40736, 11, 892, 374, 3170, 7015, 4917, 3545, 4994, 2518, 476, 18575, 13], 'total_duration': 15048512042, 'load_duration': 32180583, 'prompt_eval_count': 9, 'prompt_eval_duration': 611000000, 'eval_count': 189, 'eval_duration': 14404000000, 'telemetry': {'elapsed_time_seconds': 15.058926874999997, 'status_code': 200, 'response_size_bytes': 2241, 'usage': 'No usage data provided.'}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "def query_deepseek_api(prompt: str) -> dict:\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"deepseek-r1:32b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    response = requests.post(url, json=payload)\n",
    "    elapsed_time = time.perf_counter() - start_time\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    \n",
    "    # Try to extract usage telemetry from the response headers first.\n",
    "    usage_header = response.headers.get(\"X-Usage-Data\")\n",
    "    if usage_header:\n",
    "        try:\n",
    "            usage_info = json.loads(usage_header)\n",
    "        except json.JSONDecodeError:\n",
    "            usage_info = usage_header\n",
    "    else:\n",
    "        # Fallback to checking the JSON response body for usage info.\n",
    "        usage_info = result.get(\"usage\", \"No usage data provided.\")\n",
    "    \n",
    "    telemetry = {\n",
    "        \"elapsed_time_seconds\": elapsed_time,\n",
    "        \"status_code\": response.status_code,\n",
    "        \"response_size_bytes\": len(response.content),\n",
    "        \"usage\": usage_info\n",
    "    }\n",
    "    \n",
    "    result[\"telemetry\"] = telemetry\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"Why is the sky blue?\"\n",
    "    result = query_deepseek_api(prompt)\n",
    "    print(\"API Response:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Ingestion and Document Preparation\n",
    "\n",
    "In this section, we'll upload our documents and prepare them for indexing. Our documents can be in various formats (e.g., plain text, PDFs), but for this example we'll focus on plain text files stored in a directory.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1. Uploading Documents\n",
    "\n",
    "Place your documents in a folder (for example, a directory named `data/`). Then, use Llama Index's built-in reader to load the files. For instance, if your documents are plain text files, you can use the `SimpleDirectoryReader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-core in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (0.12.15)\n",
      "Requirement already satisfied: llama-index-readers-file in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (0.4.4)\n",
      "Collecting llama-index-llms-ollama\n",
      "  Downloading llama_index_llms_ollama-0.5.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-embeddings-huggingface\n",
      "  Downloading llama_index_embeddings_huggingface-0.5.1-py3-none-any.whl.metadata (767 bytes)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (2.0.37)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (3.11.11)\n",
      "Requirement already satisfied: dataclasses-json in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (1.0.8)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (0.2.2)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (2024.5.0)\n",
      "Requirement already satisfied: httpx in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (3.2.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (3.9.1)\n",
      "Requirement already satisfied: numpy in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (10.4.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (2.10.6)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (1.12.1)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-readers-file) (4.12.3)\n",
      "Requirement already satisfied: pandas in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-readers-file) (2.2.2)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-readers-file) (5.2.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-readers-file) (0.0.26)\n",
      "Requirement already satisfied: ollama>=0.4.3 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from llama-index-llms-ollama) (0.4.7)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.23.2)\n",
      "Collecting sentence-transformers>=2.6.1 (from llama-index-embeddings-huggingface)\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (4.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (5.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.18.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file) (2.3.1)\n",
      "Requirement already satisfied: filelock in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.15.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.2)\n",
      "Collecting minijinja>=1.0 (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
      "  Downloading minijinja-2.7.0-cp38-abi3-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: click in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from nltk>3.8.1->llama-index-core) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from nltk>3.8.1->llama-index-core) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from nltk>3.8.1->llama-index-core) (2022.3.15)\n",
      "Requirement already satisfied: anyio in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from httpx->llama-index-core) (4.8.0)\n",
      "Requirement already satisfied: certifi in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from httpx->llama-index-core) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from httpx->llama-index-core) (1.0.7)\n",
      "Requirement already satisfied: idna in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from httpx->llama-index-core) (3.3)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from pydantic>=2.8.0->llama-index-core) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from pydantic>=2.8.0->llama-index-core) (2.27.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core) (2.3.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (4.41.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.0.2)\n",
      "Requirement already satisfied: scipy in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.10.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (1.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from typing-inspect>=0.8.0->llama-index-core) (0.4.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from dataclasses-json->llama-index-core) (3.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from pandas->llama-index-readers-file) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from pandas->llama-index-readers-file) (2021.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from pandas->llama-index-readers-file) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file) (1.16.0)\n",
      "Requirement already satisfied: sympy in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.10.1)\n",
      "Requirement already satisfied: jinja2 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.11.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.4.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from anyio->httpx->llama-index-core) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from anyio->httpx->llama-index-core) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/rileymete/opt/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.2.1)\n",
      "Downloading llama_index_llms_ollama-0.5.0-py3-none-any.whl (6.7 kB)\n",
      "Downloading llama_index_embeddings_huggingface-0.5.1-py3-none-any.whl (8.9 kB)\n",
      "Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "Downloading minijinja-2.7.0-cp38-abi3-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: minijinja, llama-index-llms-ollama, sentence-transformers, llama-index-embeddings-huggingface\n",
      "Successfully installed llama-index-embeddings-huggingface-0.5.1 llama-index-llms-ollama-0.5.0 minijinja-2.7.0 sentence-transformers-3.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index-core llama-index-readers-file llama-index-llms-ollama llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 85 0 (offset 0)\n",
      "Ignoring wrong pointing object 87 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n",
      "Ignoring wrong pointing object 86 0 (offset 0)\n",
      "Ignoring wrong pointing object 88 0 (offset 0)\n",
      "Ignoring wrong pointing object 102 0 (offset 0)\n",
      "Ignoring wrong pointing object 105 0 (offset 0)\n",
      "Ignoring wrong pointing object 114 0 (offset 0)\n",
      "Ignoring wrong pointing object 116 0 (offset 0)\n",
      "Ignoring wrong pointing object 158 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 94 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 99 0 (offset 0)\n",
      "Ignoring wrong pointing object 145 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 383 documents from /Users/rileymete/Downloads/CompanyBenefits\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# Define path to the root directory containing subfolders\n",
    "directory_path = \"/Users/rileymete/Downloads/CompanyBenefits\"\n",
    "\n",
    "# Load all files, including those inside subfolders\n",
    "documents = SimpleDirectoryReader(directory_path, recursive=True).load_data()\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents from {directory_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Indexing with Llama Index – Advanced Customization\n",
    "\n",
    "In this step, we focus on how to index and customize the processing of the documents you have already uploaded (e.g., into a local folder). This section covers:\n",
    "\n",
    "1. **Loading Your Uploaded Documents**  \n",
    "   Using Llama Index’s built-in loaders to ingest files from a directory.\n",
    "\n",
    "2. **Transformations**  \n",
    "   Customizing how the documents are split into nodes (chunks) and adding metadata to improve retrieval.\n",
    "\n",
    "3. **Indexing and Querying**  \n",
    "   Building a vector index with your transformed documents and querying it for relevant information.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1. Loading Your Uploaded Documents\n",
    "\n",
    "Assuming you have already uploaded your documents into a local folder (for example, `./data`), you can use the `SimpleDirectoryReader` to load them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# Define path to the root directory containing subfolders\n",
    "directory_path = \"/Users/rileymete/Downloads/CompanyBenefits\"\n",
    "\n",
    "# Load all files, including those inside subfolders\n",
    "documents = SimpleDirectoryReader(directory_path, recursive=True).load_data()\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents from {directory_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Transformations\n",
    "\n",
    "Before indexing, you can customize how your documents are processed. This typically involves splitting them into smaller chunks (nodes) and adding metadata. You have two main options:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom text splitter configured: chunk size 512 with 10 words overlap.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Customize the text splitter with desired parameters\n",
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=10)\n",
    "\n",
    "# Option 1: Set the custom text splitter globally\n",
    "Settings.text_splitter = text_splitter\n",
    "\n",
    "print(\"Custom text splitter configured: chunk size 512 with 10 words overlap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the Vector Index Using Your Custom Transformation\n",
    "\n",
    "Now, build your vector index by passing the loaded documents along with your custom text splitter as a transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406e2ce5a6194385a67724ee37ec3a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719ee16e5c3348f5843b2324b756311c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cecfdc4e9894879aa548c59c9c6a21c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a82fc3154684f8ca4f0db4f3fc2f85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rileymete/opt/anaconda3/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5539a6699444497caa34844acb137f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60a47d914e0446d82ea1e5c0382337d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Create a local embedding model using a Hugging Face model\n",
    "embedding_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Build the index with the custom text splitter transformation and the local embedding model\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    transformations=[text_splitter],\n",
    "    embed_model=embedding_model\n",
    ")\n",
    "print(\"Custom index built successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Save the Index to Disk\n",
    "\n",
    "You can save your index to disk to avoid rebuilding it every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.save_to_disk(\"custom_index.json\")\n",
    "print(\"Index saved to disk as 'custom_index.json'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query the Index\n",
    "Finally, test your index by running a query. This cell retrieves relevant information from your indexed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the latest trends in sports news?\"\n",
    "response = index.query(query)\n",
    "\n",
    "print(\"Query Response:\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
