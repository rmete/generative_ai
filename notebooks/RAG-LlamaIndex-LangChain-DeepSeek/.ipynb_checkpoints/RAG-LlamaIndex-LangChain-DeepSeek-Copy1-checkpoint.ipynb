{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a RAG Application with Ollama deepseek-r1:32b, Llama Index, and LangChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this step-by-step guide on building a **Retrieval-Augmented Generation (RAG)** application! In this notebook, we will combine the power of retrieval methods with advanced language generation techniques. Our goal is to create a system that can retrieve relevant information from your data sources and use a state-of-the-art language model to generate insightful responses.\n",
    "\n",
    "### What is RAG?\n",
    "\n",
    "RAG stands for *Retrieval-Augmented Generation*. It is an approach that:\n",
    "- **Retrieves**: Searches and fetches relevant documents or pieces of data.\n",
    "- **Generates**: Leverages a large language model (LLM) to produce contextually accurate and insightful outputs based on the retrieved information.\n",
    "\n",
    "This technique is especially useful for tasks such as:\n",
    "- Question Answering\n",
    "- Chatbots and Conversational Agents\n",
    "- Document Summarization\n",
    "- Knowledge-based Systems\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Ollama deepseek-r1:32b**:  \n",
    "   A powerful model used for embedding and retrieval. We'll explain how to set it up and use it effectively, even if you are starting from scratch.\n",
    "\n",
    "2. **Llama Index**:  \n",
    "   A tool to efficiently build and manage indexes over your data. It simplifies organizing and querying your documents.\n",
    "\n",
    "3. **LangChain**:  \n",
    "   A versatile framework that helps integrate various components (like LLMs and indexes) into a coherent application. It provides a high-level interface to work with large language models.\n",
    "\n",
    "### What to Expect\n",
    "\n",
    "In the sections that follow, we will cover:\n",
    "- **Setting Up Your Environment**:  \n",
    "  How to install and configure Ollama deepseek-r1:32b, including a dedicated section for those who haven’t set it up yet.\n",
    "\n",
    "- **Data Ingestion & Indexing with Llama Index**:  \n",
    "  Step-by-step instructions on how to prepare your data and build an index to enable efficient retrieval.\n",
    "\n",
    "- **Integrating with LangChain**:  \n",
    "  How to tie everything together by interfacing the index with your language model for retrieval-augmented generation.\n",
    "\n",
    "- **Example Use Cases & Exercises**:  \n",
    "  Practical code snippets and exercises to help you apply what you’ve learned in real-world scenarios.\n",
    "\n",
    "By the end of this notebook, you’ll have a clear understanding of how to build and deploy your own RAG application, empowering you to tackle complex information retrieval and generation tasks.\n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup\n",
    "\n",
    "In this section, we will prepare our environment by installing the necessary Python libraries, setting up Ollama with the **deepseek-r1:32b** model, and verifying that our setup is working correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Installing Required Libraries\n",
    "\n",
    "Our RAG application will leverage the following key Python packages:\n",
    "- **LangChain**: For integrating with large language models.\n",
    "- **Llama Index**: For building and querying document indexes.\n",
    "- **Requests**: For making HTTP calls (useful if you interact with an API).\n",
    "\n",
    "Open your terminal or command prompt and run the following command to install these packages:\n",
    "\n",
    "```bash\n",
    "pip install langchain llama-index requests\n",
    "```\n",
    "\n",
    "For the latest installation instructions or updates, please refer to the official documentation:\n",
    "- [LangChain GitHub Repository](https://github.com/hwchase17/langchain)\n",
    "- [Llama Index Documentation](https://gpt-index.readthedocs.io/en/latest/)\n",
    "- [Requests Documentation](https://requests.readthedocs.io/en/latest/)\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2. Setting Up Ollama and deepseek-r1:32b\n",
    "\n",
    "**Ollama** is a platform that enables you to run large language models locally. In our case, we will use it to host the **deepseek-r1:32b** model.\n",
    "\n",
    "### Steps to Set Up:\n",
    "\n",
    "1. **Install Ollama**:  \n",
    "   Visit the [Ollama website](https://ollama.com) and follow the installation instructions for your operating system.\n",
    "\n",
    "2. **Download deepseek-r1:32b**:  \n",
    "   If you haven’t already downloaded the model, you can pull it via the Ollama CLI:\n",
    "   ```bash\n",
    "   ollama pull deepseek-r1:32b\n",
    "   ```\n",
    "\n",
    "3. **Start the Model**:  \n",
    "   Ensure that the model is running on your machine. The exact steps might vary depending on your installation. Consult the [Ollama documentation](https://ollama.com/docs) for detailed guidance.\n",
    "\n",
    "> **Note:** If you prefer using an HTTP API (if provided by your Ollama installation) over the CLI, instructions will be provided later in the notebook.\n",
    "\n",
    "Once you have completed these steps, your local deepseek-r1:32b model should be ready for use.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 2.3. Verifying the Setup\n",
    "\n",
    "Before proceeding, let’s verify that both our Python environment and deepseek-r1:32b are working as expected.\n",
    "\n",
    "#### 2.3.1. Verify Python Package Installation\n",
    "\n",
    "Run the following code snippet in a Python cell to ensure that all necessary packages are installed and importable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import llama_index\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2. Verify deepseek-r1:32b via the Ollama CLI\n",
    "\n",
    "If you’re using the Ollama CLI to interact with deepseek-r1:32b, you can run a quick test. Create a helper function in your notebook that sends a prompt to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def query_deepseek(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Sends a prompt to the deepseek-r1:32b model via the Ollama CLI.\n",
    "    \"\"\"\n",
    "    command = [\"ollama\", \"run\", \"deepseek-r1:32b\", prompt]\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"Error calling deepseek-r1:32b: {result.stderr}\")\n",
    "    \n",
    "    return result.stdout.strip()\n",
    "\n",
    "# Test the function:\n",
    "try:\n",
    "    test_response = query_deepseek(\"Hello, deepseek-r1:32b! Please confirm you are running.\")\n",
    "    print(\"Model Response:\", test_response)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3. (Optional) Verify deepseek-r1:32b via an HTTP API\n",
    "\n",
    "If your Ollama installation provides an HTTP API endpoint, you can test it using the `requests` library. Adjust the API endpoint as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def query_deepseek_api(prompt: str) -> dict:\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"deepseek-r1:32b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    response = requests.post(url, json=payload)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "# Example usage\n",
    "result = query_deepseek_api(\"Why is the sky blue?\")\n",
    "print(\"API Response:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.get('response')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Indexing with Llama Index – Advanced Customization\n",
    "\n",
    "In this step, we focus on how to index and customize the processing of the documents you have already uploaded (e.g., into a local folder). This section covers:\n",
    "\n",
    "1. **Loading Your Uploaded Documents**  \n",
    "   Using Llama Index’s built-in loaders to ingest files from a directory.\n",
    "\n",
    "2. **Transformations**  \n",
    "   Customizing how the documents are split into nodes (chunks) and adding metadata to improve retrieval.\n",
    "\n",
    "3. **Indexing and Querying**  \n",
    "   Building a vector index with your transformed documents and querying it for relevant information.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1. Loading Your Uploaded Documents\n",
    "\n",
    "Assuming you have already uploaded your documents into a local folder (for example, `./data`), you can use the `SimpleDirectoryReader` to load them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index-core llama-index-readers-file llama-index-llms-ollama llama-index-embeddings-huggingface transformers requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# Define path to the root directory containing subfolders\n",
    "directory_path = \"./data\"\n",
    "\n",
    "# Load all files, including those inside subfolders\n",
    "documents = SimpleDirectoryReader(directory_path, recursive=True).load_data()\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents from {directory_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Transformations\n",
    "\n",
    "Before indexing, you can customize how your documents are processed. This typically involves splitting them into smaller chunks (nodes) and adding metadata. You have two main options:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Customize the text splitter with desired parameters\n",
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=10)\n",
    "\n",
    "# Option 1: Set the custom text splitter globally\n",
    "Settings.text_splitter = text_splitter\n",
    "\n",
    "print(\"Custom text splitter configured: chunk size 512 with 10 words overlap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Building the Vector Index with Hugging Face API**  \n",
    "This approach uses the Hugging Face API (`gte-large` model) to generate embeddings. A custom text splitter transformation is applied to segment the documents before indexing them. This method requires an internet connection and an API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceInferenceAPIEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "\n",
    "# Example using Hugging Face API\n",
    "embedding_model = HuggingFaceInferenceAPIEmbedding(\n",
    "    model_name=\"thenlper/gte-large\",\n",
    "    use_auth_token=\"<auth-token>\"\n",
    ")\n",
    "\n",
    "# Build the index with the custom text splitter transformation and the local embedding model\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents[0:1],\n",
    "    transformations=[text_splitter],\n",
    "    embed_model=embedding_model\n",
    ")\n",
    "print(\"Custom index built successfully!\")\n",
    "\n",
    "# Save index to disk\n",
    "storage_dir = \"custom_index_storage\"\n",
    "index.storage_context.persist(persist_dir=storage_dir)\n",
    "print(f\"Index saved to disk at: {storage_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Building the Vector Index with Ollama Mistral**  \n",
    "This approach uses the **Mistral model** via Ollama to generate embeddings locally. A custom text splitter transformation is applied before indexing the documents. This method runs entirely offline but requires downloading the model in advance.\n",
    "\n",
    "**Note: Choose only one approach based on your needs:**  \n",
    "- Use **Hugging Face API** if you want cloud-based embeddings and don't mind API costs. There is also a free tier but it has rate limits.\n",
    "- Use **Ollama Mistral** if you want a **fully local** and **free** solution but have the necessary compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.storage import StorageContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# Initialize the Ollama embedding model\n",
    "# Note: You may need to pull mistral model if you don't have it already\n",
    "embedding_model = OllamaEmbedding(model_name=\"mistral\")\n",
    "\n",
    "# Create index with Ollama Mistral embedding\n",
    "index = VectorStoreIndex.from_documents(documents, transformations=[text_splitter], embed_model=embedding_model)\n",
    "\n",
    "# Save index\n",
    "storage_dir = \"mistral_index_storage\"\n",
    "index.storage_context.persist(persist_dir=storage_dir)\n",
    "print(f\"✅ Index saved at: {storage_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Querying the Vector Index with Hugging Face API**  \n",
    "This approach retrieves relevant information using the **Hugging Face API** embeddings. It requires an internet connection and an API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceInferenceAPIEmbedding\n",
    "from llama_index.core import load_index_from_storage, StorageContext\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "def query_index_hf(query, index=None, storage_dir=\"huggingface_index_storage\"):\n",
    "    \"\"\"Query the index using Hugging Face API embeddings.\"\"\"\n",
    "    if index is None:\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=storage_dir)\n",
    "        index = load_index_from_storage(storage_context)\n",
    "\n",
    "    embedding_model = HuggingFaceInferenceAPIEmbedding(model_name=\"thenlper/gte-large\")\n",
    "    retriever = index.as_retriever()\n",
    "    query_engine = RetrieverQueryEngine(retriever=retriever)\n",
    "\n",
    "    return query_engine.query(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Querying the Vector Index with Ollama Mistral**  \n",
    "This approach retrieves relevant information using the **Mistral model** via Ollama. It runs locally and requires the model to be downloaded beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import load_index_from_storage, StorageContext\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "def query_index_ollama(query, index=None, storage_dir=\"mistral_index_storage\"):\n",
    "    \"\"\"Query the index using Ollama Mistral embeddings.\"\"\"\n",
    "    if index is None:\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=storage_dir)\n",
    "        index = load_index_from_storage(storage_context)\n",
    "\n",
    "    embedding_model = OllamaEmbedding(model_name=\"mistral\")\n",
    "    retriever = index.as_retriever()\n",
    "    query_engine = RetrieverQueryEngine(retriever=retriever)\n",
    "\n",
    "    return query_engine.query(query)\n",
    "\n",
    "\n",
    "query_index_ollama('What are my dental benefits', index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 4: Implementing a RAG System with LlamaIndex, LangChain, and DeepSeek**\n",
    "In this step, we integrate **LlamaIndex**, **LangChain**, and **DeepSeek** to build a **Retrieval-Augmented Generation (RAG) application**.  \n",
    "\n",
    "### **How It Works**\n",
    "1. **Retrieve Relevant Documents**  \n",
    "   - We use `LlamaIndexRetriever` with **Ollama Mistral** embeddings to fetch relevant documents.  \n",
    "\n",
    "2. **Format the Query with Context**  \n",
    "   - The retrieved content is formatted into a structured prompt to guide DeepSeek in generating a better response.  \n",
    "\n",
    "3. **Generate a Response with DeepSeek**  \n",
    "   - We use `DeepSeekLLM`, a LangChain-compatible LLM wrapper, to process the prompt and generate an answer.  \n",
    "   - The response is split into:\n",
    "     - **Reasoning:** The model's thought process.\n",
    "     - **Answer:** The final extracted response.  \n",
    "\n",
    "### **Key Components**\n",
    "- **LlamaIndexRetriever** → Retrieves context using **LlamaIndex** and **Ollama Mistral** embeddings.  \n",
    "- **DeepSeekLLM** → Calls a locally running **DeepSeek-R1** model using an API.  \n",
    "- **LangChain Integration** → Provides flexibility for chaining retrieval and generation models.  \n",
    "- **`generate_response(query)`** → Orchestrates retrieval and response generation to deliver grounded answers.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.llms.base import LLM\n",
    "from llama_index.core import load_index_from_storage, StorageContext\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.settings import Settings\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "# Disable OpenAI API key dependency\n",
    "Settings.embed_model = None\n",
    "\n",
    "# --- DeepSeek LLM Class ---\n",
    "class DeepSeekLLM:\n",
    "    \"\"\"A simple wrapper for a locally hosted DeepSeek model.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_url: str = \"http://localhost:11434/api/generate\", model_name: str = \"deepseek-r1:32b\"):\n",
    "        self.api_url = api_url\n",
    "        self.model_name = model_name\n",
    "    \n",
    "    def query(self, prompt: str) -> Dict[str, str]:\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        response = requests.post(self.api_url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        try:\n",
    "            raw_response = response.json()\n",
    "            full_response = raw_response.get(\"response\", \"No response generated.\").strip()\n",
    "            \n",
    "            # Extract reasoning and answer based on <think> tags\n",
    "            if \"<think>\" in full_response and \"</think>\" in full_response:\n",
    "                reasoning_start = full_response.find(\"<think>\") + len(\"<think>\")\n",
    "                reasoning_end = full_response.find(\"</think>\")\n",
    "                reasoning = full_response[reasoning_start:reasoning_end].strip()\n",
    "                answer = full_response[reasoning_end + len(\"</think>\"):].strip()\n",
    "            else:\n",
    "                reasoning, answer = full_response, \"No explicit response found.\"\n",
    "            \n",
    "            return {\n",
    "                \"prompt\": prompt,\n",
    "                \"reasoning\": reasoning,\n",
    "                \"answer\": answer,\n",
    "                \"relevant_documents\": raw_response.get(\"context\", [])\n",
    "            }\n",
    "        except requests.exceptions.JSONDecodeError:\n",
    "            return {\n",
    "                \"prompt\": prompt,\n",
    "                \"reasoning\": \"Error: Unable to decode DeepSeek response.\",\n",
    "                \"answer\": \"\",\n",
    "                \"relevant_documents\": []\n",
    "            }\n",
    "\n",
    "# --- Retriever Class ---\n",
    "class LlamaIndexRetriever:\n",
    "    \"\"\"Retrieves relevant documents using LlamaIndex and Ollama Mistral embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, index=None, storage_dir: str = \"mistral_index_storage\", num_passages: int = 15):\n",
    "        self.storage_dir = storage_dir\n",
    "        self.embedding_model = OllamaEmbedding(model_name=\"mistral\", api_key=None)\n",
    "        self.num_passages = num_passages  # Number of passages to retrieve\n",
    "\n",
    "        if index is None:\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=storage_dir)\n",
    "            self.index = load_index_from_storage(storage_context, embed_model=self.embedding_model)\n",
    "        else:\n",
    "            self.index = index\n",
    "        \n",
    "        self.retriever = self.index.as_retriever(similarity_top_k=self.num_passages)\n",
    "        \n",
    "    def retrieve(self, query: str) -> List[str]:\n",
    "        query_engine = RetrieverQueryEngine(retriever=self.retriever)\n",
    "        response = query_engine.query(query)\n",
    "        \n",
    "        # Extract text from retrieved nodes\n",
    "        documents = [node.text for node in response.source_nodes]\n",
    "        return documents\n",
    "\n",
    "# --- End-to-End Execution ---\n",
    "def generate_response(query: str, index=None, num_passages: int = 15) -> Dict[str, str]:\n",
    "    retriever = LlamaIndexRetriever(index=index, num_passages=num_passages)\n",
    "    deepseek_llm = DeepSeekLLM()\n",
    "    \n",
    "    retrieved_texts = retriever.retrieve(query)\n",
    "    \n",
    "    if not retrieved_texts:\n",
    "        return {\"prompt\": query, \"reasoning\": \"No relevant context found to answer the question.\", \"answer\": \"\", \"relevant_documents\": []}\n",
    "    \n",
    "    formatted_context = \"\\n\\n\".join(retrieved_texts)\n",
    "    \n",
    "    prompt = (f\"Given the following context, answer the question concisely.\\n\\n\"\n",
    "              f\"### Context:\\n{formatted_context}\\n\\n\"\n",
    "              f\"### Question: {query}\\n\"\n",
    "              f\"### Answer:\")\n",
    "    \n",
    "    response_dict = deepseek_llm.query(prompt)\n",
    "    response_dict[\"relevant_documents\"] = retrieved_texts  # Ensure full document texts are included\n",
    "    \n",
    "    return response_dict\n",
    "\n",
    "# Example usage\n",
    "query = \"Summarize my HMO Medical Plan\"\n",
    "response = generate_response(query, index=None, num_passages=15)\n",
    "print(f\"DeepSeek R1 Response:\\n{response['answer']}\\n\\n\")\n",
    "print(f\"DeepSeek R1 Reasoning:\\n{response['reasoning']}\\n\\n\")\n",
    "print(f\"Retrieved Documents:\\n{response['relevant_documents']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
