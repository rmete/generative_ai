{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY7Y-gIBFluw"
      },
      "source": [
        "# **CourseraRAG: AI-Powered Study Assistant**\n",
        "\n",
        "### **Project Overview**\n",
        "\n",
        "The **CourseraRAG: AI-Powered Study Assistant** is a project designed to transform the study process through state-of-the-art technology. It integrates three key elements:\n",
        "\n",
        "1. **Retrieval System**  \n",
        "   A cornerstone of this project is its advanced retrieval system. This system is uniquely configured to incorporate a comprehensive corpus of information, which includes every lecture transcript and each chapter of the course textbook, converted into text files. By integrating these diverse and rich resources, the system is exceptionally equipped to identify and present the most relevant documents in response to specific user queries. This approach ensures that the information retrieved is not only pertinent but also encompasses a broad spectrum of educational materials, facilitating focused and effective learning.\n",
        "\n",
        "2. **Generative AI (Powered by OpenAI APIs)**  \n",
        "   Utilizing OpenAI's APIs, this segment of the project generates coherent and contextually accurate answers based on the documents retrieved. The use of OpenAI's advanced AI technology guarantees that the responses are relevant, reliable, and grounded in the substantial database of lecture transcripts and textbook content, thereby elevating the quality and accuracy of the information provided.\n",
        "\n",
        "3. **Interactive User Interface**  \n",
        "   The user interface is the heart of **CourseraRAG**. It is crafted to be engaging and user-friendly, enabling users to seamlessly pose questions and receive AI-generated answers. This interactive platform is the gateway to the sophisticated capabilities of both the Retrieval System and Generative AI, making it a dynamic and accessible educational tool.\n",
        "\n",
        "### **Project Goals**\n",
        "\n",
        "- To deliver efficient and targeted learning by providing access to a rich database of lecture transcripts and textbook content.\n",
        "- To enhance understanding and retention through AI-generated answers, leveraging the comprehensive corpus of educational materials and the power of OpenAI's APIs.\n",
        "- To provide an intuitive and interactive tool that caters to the diverse needs of learners and educators.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N7fUYkYKKOV"
      },
      "source": [
        "## Step 1: Conversion of Textbook PDF into Chapter-wise Text Files\n",
        "\n",
        "This initial step involves the meticulous conversion of the textbook from its PDF format into individual text files, each corresponding to a separate chapter. This process is designed to ensure that each chapter is distinctly segmented, facilitating ease of access and reference in the subsequent stages of the project.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTm5LiwxNErN",
        "outputId": "2baf0660-f1b3-484a-ce54-fec2204cf2d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.10.3-py3-none-any.whl (48 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m41.0/49.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.0/49.0 kB\u001b[0m \u001b[31m932.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20221105 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.26.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (41.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n",
            "Installing collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20221105 pdfplumber-0.10.3 pypdfium2-4.26.0\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting openai\n",
            "  Downloading openai-1.8.0-py3-none-any.whl (222 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.3/222.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typing-extensions, h11, httpcore, httpx, openai\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 openai-1.8.0 typing-extensions-4.9.0\n"
          ]
        }
      ],
      "source": [
        "%pip install pdfplumber\n",
        "%pip install pytesseract\n",
        "%pip install PyPDF2\n",
        "%pip install openai\n",
        "import pdfplumber\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import PyPDF2\n",
        "import re\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIeXwSpXU5g_",
        "outputId": "2312ad9d-e05f-4d55-9a0e-80f85a5345c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OoPRIoYJS_D"
      },
      "outputs": [],
      "source": [
        "def insert_spaces(text):\n",
        "    # Pattern to identify places where a lowercase letter is followed by an uppercase letter\n",
        "    pattern = re.compile(r'(?<=[a-z])(?=[A-Z])')\n",
        "\n",
        "    # Insert a space at each identified position\n",
        "    return pattern.sub(' ', text)\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "            # Check for encryption and try to decrypt\n",
        "            if reader.is_encrypted:\n",
        "                try:\n",
        "                    reader.decrypt('')\n",
        "                except Exception as e:\n",
        "                    print(f\"Unable to decrypt PDF: {e}\")\n",
        "                    return None\n",
        "\n",
        "            text = ''\n",
        "\n",
        "            # Extract text from each page\n",
        "            for page in reader.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    # Add logic here if you need to clean or format the text\n",
        "                    text += page_text + '\\n'\n",
        "            return text\n",
        "    except FileNotFoundError:\n",
        "        print(\"File not found. Please check the file path.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "def chunk_text_by_chapter_and_save(text, chapter_titles, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    current_chapter = None\n",
        "    chapter_contents = []\n",
        "\n",
        "    for line in text.split('\\n'):\n",
        "        # Check if the line matches or partially matches any chapter title\n",
        "        for title in chapter_titles:\n",
        "            if title.startswith(line) or line.startswith(title):\n",
        "                if current_chapter:\n",
        "                    save_chapter_to_file(current_chapter, '\\n'.join(chapter_contents), output_folder)\n",
        "                current_chapter = title\n",
        "                chapter_contents = []\n",
        "                break\n",
        "        else:  # This else corresponds to the for-loop\n",
        "            chapter_contents.append(line)\n",
        "\n",
        "    if current_chapter:\n",
        "        save_chapter_to_file(current_chapter, '\\n'.join(chapter_contents), output_folder)\n",
        "\n",
        "def save_chapter_to_file(chapter_title, content, output_folder):\n",
        "    filename = f\"{chapter_title}.txt\".replace(' ', '_').replace('/', '_')\n",
        "    file_path = os.path.join(output_folder, filename)\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCOhTiltMB8Q"
      },
      "outputs": [],
      "source": [
        "chapter_titles = [\n",
        "    \"1Introduction\",\n",
        "    \"2Background\",\n",
        "    \"3Text Data Understanding\",\n",
        "    \"4META: A Unified Toolkit\",\n",
        "    \"5Overview of Text\",\n",
        "    \"6Retrieval Models\",\n",
        "    \"7Feedback\",\n",
        "    \"8Search Engine\",\n",
        "    \"9Search Engine Evaluation\",\n",
        "    \"10Web Search\",\n",
        "    \"11Recommender Systems\",\n",
        "    \"12Overview of Text\",\n",
        "    \"13Word Association Mining\",\n",
        "    \"14Text Clustering\",\n",
        "    \"15Text Categorization\",\n",
        "    \"16Text Summarization\",\n",
        "    \"17Topic Analysis\",\n",
        "    \"18Opinion Mining and\",\n",
        "    \"19Joint Analysis of Text\",\n",
        "    \"20Toward A Unified System for Text Management and Analysis\"\n",
        "]\n",
        "\n",
        "\n",
        "pdf_path = '/content/drive/MyDrive/CS_410/textbook_410.pdf'\n",
        "text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if text:\n",
        "    chunk_text_by_chapter_and_save(text, chapter_titles, \"files/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd4AsiEXQYmW"
      },
      "source": [
        "## Step 2: File Verification and Inventory\n",
        "\n",
        "This step is crucial in ensuring the completeness and readiness of our resources. It involves a thorough verification process to confirm the presence of all necessary files. The key components to be verified are:\n",
        "\n",
        "- **Overview Files:** These files should contain guiding questions and key concepts. It's essential to ensure that each overview file is complete and accurately reflects the course material.\n",
        "\n",
        "- **Lecture Transcripts:** We need to have transcripts for each week's lecture. This step includes checking that each transcript is available, legible, and correctly corresponds to the respective week's content.\n",
        "\n",
        "- **Textbook Chapters:** Different chapters from our textbook should be available as individual text files. The verification process here involves confirming that each chapter is properly extracted, correctly labeled, and includes all the relevant content.\n",
        "\n",
        "This comprehensive verification ensures that all the critical educational resources are in place and correctly organized for the subsequent stages of our project.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-Xt8b6WSdDN",
        "outputId": "e29d2bda-3671-4c67-bbf5-f46cbafd4a50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File: Week1_Overview.txt\n",
            "Length: 605 characters\n",
            "Goals and Objectives:\n",
            "- • Understand what cloud computing is and why it is important.\n",
            "- • Get a picture of the economics of cloud computing.\n",
            "- • Learn about Big Data (much more about Big Data in the second part of this course).\n",
            "- Key Phrases/Concepts\n",
            "- • Cloud computing\n",
            "- • Big Data\n",
            "- • Cloudonmics\n",
            "- • Software defined architecture\n",
            "- • IaaS: Infrastructure as a Service\n",
            "\n",
            "Guiding Questions:\n",
            "\n",
            "Key Phrases and Concepts:\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "The textbook chapters directory /content/drive/MyDrive/UIUC/CS_498/Week1/Textbook does not exist.\n",
            "\n",
            "\n",
            "1_1_Cloud_Computing_Introduction.txt\n",
            "1_2_Cloudonomics_Part_1.txt\n",
            "1_3_Cloudonomics_Part_2.txt\n",
            "1_4_Big_Data.txt\n",
            "1_5_Summary_to_Cloud_Introduction.txt\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "# Function to extract content between two headings\n",
        "def extract_content(heading, text, next_heading=None):\n",
        "    if next_heading:\n",
        "        pattern = re.compile(rf\"{heading}\\n(.*?)\\n{next_heading}\", re.DOTALL)\n",
        "    else:\n",
        "        pattern = re.compile(rf\"{heading}\\n(.*?)(?=\\n[A-Z][a-z])\", re.DOTALL)\n",
        "    match = pattern.search(text)\n",
        "    content = match.group(1).strip() if match else \"\"\n",
        "    return [line.strip() for line in content.split('\\n') if line.strip()]\n",
        "\n",
        "# Function to process overview files in a specified directory\n",
        "def process_overviews(directory):\n",
        "    files = sorted(os.listdir(directory))\n",
        "    for file in files:\n",
        "        file_path = os.path.join(directory, file)\n",
        "        if os.path.isfile(file_path):\n",
        "            with open(file_path, 'r') as f:\n",
        "                text = f.read()\n",
        "\n",
        "            goals_and_objectives = extract_content(\"Goals and Objectives\", text, \"Guiding Questions\")\n",
        "            guiding_questions = extract_content(\"Guiding Questions\", text, \"Key Phrases and Concepts\")\n",
        "            key_phrases_and_concepts = extract_content(\"Key Phrases and Concepts\", text)\n",
        "\n",
        "            print(f\"File: {file}\")\n",
        "            print(f\"Length: {len(text)} characters\")\n",
        "            print(\"Goals and Objectives:\")\n",
        "            for item in goals_and_objectives:\n",
        "                print(\"-\", item)\n",
        "            print(\"\\nGuiding Questions:\")\n",
        "            for item in guiding_questions:\n",
        "                print(\"-\", item)\n",
        "            print(\"\\nKey Phrases and Concepts:\")\n",
        "            for item in key_phrases_and_concepts:\n",
        "                print(\"-\", item)\n",
        "            print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "# Function to list textbook chapters or lecture transcripts\n",
        "def list_files(directory, title):\n",
        "    files = sorted(os.listdir(directory))\n",
        "    print(f\"{title}:\")\n",
        "    for file in files:\n",
        "        file_path = os.path.join(directory, file)\n",
        "        if os.path.isfile(file_path):\n",
        "            with open(file_path, 'r') as f:\n",
        "                text = f.read()\n",
        "            print(f\"{file} - Length: {len(text)} characters\")\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "# Function to process lecture transcripts in weekly folders\n",
        "def process_lecture_transcripts(directory):\n",
        "    for week in sorted(os.listdir(directory)):\n",
        "        week_path = os.path.join(directory, week)\n",
        "        if os.path.isdir(week_path):\n",
        "            print(f\"Processing {week} Transcripts:\")\n",
        "            list_files(week_path, f\"{week} Transcripts\")\n",
        "            print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "# Directory paths\n",
        "overview_directory = \"/content/drive/MyDrive/CS_410/Text Files/Overview\"\n",
        "chapter_directory = \"/content/drive/MyDrive/CS_410/Text Files/Textbook\"\n",
        "lecture_directory = \"/content/drive/MyDrive/CS_410/Text Files/Lectures\"\n",
        "\n",
        "# Check if directories exist and process files\n",
        "if os.path.exists(overview_directory):\n",
        "    process_overviews(overview_directory)\n",
        "else:\n",
        "    print(f\"The overview directory {overview_directory} does not exist.\")\n",
        "\n",
        "if os.path.exists(chapter_directory):\n",
        "    list_files(chapter_directory, \"Textbook Chapters\")\n",
        "else:\n",
        "    print(f\"The textbook chapters directory {chapter_directory} does not exist.\")\n",
        "\n",
        "if os.path.exists(lecture_directory):\n",
        "    process_lecture_transcripts(lecture_directory)\n",
        "else:\n",
        "    print(f\"The lecture transcripts directory {lecture_directory} does not exist.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyU2PrxzeltR"
      },
      "source": [
        "## Step 3: Advanced Text Analysis and Retrieval System\n",
        "\n",
        "This section of the project is dedicated to the development of an advanced text analysis and retrieval system. It encompasses several key functionalities designed to streamline the process of querying and extracting valuable insights from a comprehensive set of educational resources. The main components and their functionalities include:\n",
        "\n",
        "- **Content Extraction from Overview Files:** Utilizing a custom function to parse and extract guiding questions from overview files. This ensures a focused approach in identifying key areas of study and topics of interest.\n",
        "\n",
        "- **Preprocessing of Textual Data:** Implementation of a preprocessing routine involving lemmatization and removal of stopwords. This step is crucial for standardizing the text data, enhancing the effectiveness of subsequent analysis.\n",
        "\n",
        "- **Lecture Transcripts and Textbook Chapters Processing:** Systematic processing of lecture transcripts and textbook chapters, converting them into a format suitable for advanced text analysis. This includes organizing lectures by weeks and chapters by their specific content.\n",
        "\n",
        "- **TF-IDF Vectorization:** Application of the Term Frequency-Inverse Document Frequency (TF-IDF) vectorization to transform the textual data into a numerical format. This transformation is vital for enabling sophisticated similarity comparisons.\n",
        "\n",
        "- **Cosine Similarity-Based Document Retrieval:** Utilization of cosine similarity measures to identify the most relevant documents in response to a query. This component is adept at retrieving the top matching lecture transcripts and textbook chapters, tailored to the specifics of each query.\n",
        "\n",
        "- **Results Presentation:** Displaying the top matching documents, including lecture transcripts and the most relevant textbook chapter for each query. This provides users with immediate access to the most pertinent information, fostering an efficient and targeted learning experience.\n",
        "\n",
        "This comprehensive system ensures that students and educators can swiftly locate the most relevant information, thereby enhancing the overall effectiveness of the learning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFsmO6DtZgn5",
        "outputId": "91d76155-196d-4268-b8e3-2ede2eb9a7fe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import nltk\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def extract_content(heading, text, next_heading=None):\n",
        "    if next_heading:\n",
        "        # The (?s) flag makes '.' match any character including newline\n",
        "        pattern = re.compile(rf\"{re.escape(heading)}\\s*\\n(?s)(.*?)\\n{re.escape(next_heading)}\", re.DOTALL)\n",
        "    else:\n",
        "        # The [^\\n•] pattern matches any character except newline or bullet points\n",
        "        pattern = re.compile(rf\"{re.escape(heading)}\\s*\\n(?s)(.*?)(?=\\n[A-Z][a-z])\", re.DOTALL)\n",
        "    match = pattern.search(text)\n",
        "    content = match.group(1).strip() if match else \"\"\n",
        "    return [line.strip() for line in content.split('\\n') if line.strip()]\n",
        "\n",
        "# Function to process overview files and create a list of all guiding questions\n",
        "def process_overviews_and_extract_questions(directory):\n",
        "    all_guiding_questions = []\n",
        "    files = sorted(os.listdir(directory))\n",
        "    for file in files:\n",
        "        file_path = os.path.join(directory, file)\n",
        "        if os.path.isfile(file_path):\n",
        "            with open(file_path, 'r') as f:\n",
        "                text = f.read()\n",
        "\n",
        "            guiding_questions = extract_content(\"Guiding Questions\", text, \"Key Phrases and Concepts\")\n",
        "            all_guiding_questions.extend(guiding_questions)\n",
        "\n",
        "    return all_guiding_questions\n",
        "\n",
        "def preprocess(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = text.lower().split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stopwords.words('english')]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Directories\n",
        "overview_directory = \"/content/drive/MyDrive/UIUC/CS_498/Overview\"\n",
        "#chapter_directory = \"/content/drive/MyDrive/UIUC/CS_498/Week1/Textbook\"\n",
        "lecture_directory = \"/content/drive/MyDrive/UIUC/CS_498/Lectures\"\n",
        "\n",
        "# Extract and preprocess overview questions\n",
        "overview_questions = process_overviews_and_extract_questions(overview_directory)\n",
        "\n",
        "# Read and preprocess transcripts and textbook chapters\n",
        "documents = []\n",
        "document_names = []\n",
        "\n",
        "# Process lecture transcripts\n",
        "for week_folder in sorted(os.listdir(lecture_directory)):\n",
        "    week_path = os.path.join(lecture_directory, week_folder)\n",
        "    if os.path.isdir(week_path):\n",
        "        for filename in os.listdir(week_path):\n",
        "            file_path = os.path.join(week_path, filename)\n",
        "            if filename.endswith(\".txt\"):\n",
        "                with open(file_path, 'r') as file:\n",
        "                    documents.append(preprocess(file.read()))\n",
        "                    document_names.append(f\"{week_folder}/{filename}\")\n",
        "\n",
        "# Process textbook chapters\n",
        "for filename in os.listdir(chapter_directory):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        with open(os.path.join(chapter_directory, filename), 'r') as file:\n",
        "            documents.append(preprocess(file.read()))\n",
        "            document_names.append(f\"Textbook/{filename}\")\n",
        "\n",
        "# TfidfVectorizer with preprocessing\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Function to find top matching documents\n",
        "def find_top_documents(query, top_n=3, is_textbook=False):\n",
        "    query_tfidf = vectorizer.transform([query])\n",
        "    cosine_similarities = cosine_similarity(query_tfidf, tfidf_matrix).flatten()\n",
        "\n",
        "    if is_textbook:\n",
        "        # Filter to include only textbook chapters\n",
        "        textbook_indices = [i for i, doc_name in enumerate(document_names) if \"Textbook/\" in doc_name]\n",
        "        textbook_similarities = [cosine_similarities[i] for i in textbook_indices]\n",
        "        top_indices = sorted(range(len(textbook_similarities)), key=lambda i: textbook_similarities[i], reverse=True)[:top_n]\n",
        "        top_indices = [textbook_indices[i] for i in top_indices]  # Map back to original indices\n",
        "    else:\n",
        "        top_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
        "\n",
        "    results = [(document_names[i], cosine_similarities[i]) for i in top_indices]\n",
        "    return results[0] if is_textbook else results\n",
        "\n",
        "\n",
        "# Find and print top documents for each query\n",
        "for query in overview_questions:\n",
        "    top_lectures = find_top_documents(query)\n",
        "    top_chapter = find_top_documents(query, top_n=1, is_textbook=True)\n",
        "\n",
        "    print(f\"Query: {query}\")\n",
        "    print(\"Top Lecture Transcripts:\")\n",
        "    for doc, score in top_lectures:\n",
        "        if \"Textbook\" not in doc:\n",
        "            print(f\"Matching document: {doc} with score {float(score):.4f}\")\n",
        "\n",
        "    print(\"\\nTop Textbook Chapter:\")\n",
        "    if top_chapter:\n",
        "        chapter_doc, chapter_score = top_chapter\n",
        "        print(f\"Matching chapter: {chapter_doc} with score {float(chapter_score):.4f}\")\n",
        "    else:\n",
        "        print(\"No matching chapter found.\")\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuvEOoEiQ9tu",
        "outputId": "17676946-50c6-4a0c-cc83-b352a79fb891"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "overview_questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPGZcxf6es5w"
      },
      "source": [
        "## Step 4: Parallelized Query Processing with GPT-4 and Contextual Document Retrieval\n",
        "\n",
        "This section of the project focuses on efficiently answering guiding questions by leveraging the advanced capabilities of GPT-4, in conjunction with a context-based retrieval system. Utilizing parallel processing and the integration of contextual documents, this system aims to provide comprehensive and relevant answers to key questions derived from educational materials. Key features of this section include:\n",
        "\n",
        "- **GPT-4 Query Function**: Utilizes OpenAI's GPT-4 to generate answers for the provided queries. This function forms the core of the query-answering mechanism, leveraging the advanced language understanding capabilities of GPT-4.\n",
        "\n",
        "- **Contextual Information Gathering**: Before querying GPT-4, the system gathers relevant contextual information from a set of pre-identified documents. This includes the top lecture transcripts and the most relevant textbook chapters related to each query, ensuring that the responses are well-informed and pertinent.\n",
        "\n",
        "- **Preprocessing and TF-IDF Vectorization**: Implements preprocessing routines and TF-IDF vectorization to transform textual data into a suitable format for analysis, enhancing the effectiveness of document retrieval based on query relevance.\n",
        "\n",
        "- **Parallelized Processing**: Employs Python's `concurrent.futures.ThreadPoolExecutor` for parallel processing of multiple queries. This approach significantly improves efficiency, especially when dealing with multiple queries and large volumes of data.\n",
        "\n",
        "- **Dynamic Response Generation**: For each guiding question, the system dynamically generates a prompt that includes the question and its associated contextual documents. This prompt is then used to query GPT-4, ensuring that the AI's response is informed by the most relevant and recent academic content.\n",
        "\n",
        "- **Comprehensive Output**: The output for each query includes the guiding question, the documents used for context (both lecture transcripts and textbook chapters), and the answer generated by GPT-4. This structure provides a clear and thorough understanding of how each response was derived.\n",
        "\n",
        "This system represents a sophisticated approach to automated query answering in educational settings, combining state-of-the-art AI with contextually rich academic resources to deliver insightful and accurate responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfyNoyYPgNsp",
        "outputId": "c43107a2-fcdb-4201-e1fc-e840ea509b3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Develop your answers to the following guiding questions while watching the video lectures throughout the week.\n",
            "Documents used: Week12/12_5_Contextual_Text_Mining_Contextual_Probabilistic_Latent_Semantic_Analysis.txt, Week4/4_1_Probabilistic_Retrieval_Model_Basic_Idea.txt, Week1/1_3_Text_Retrieval_Problem.txt, Textbook/3Text_Data_Understanding.txt\n",
            "Answer: These documents provide a detailed insight into several concepts essential to understanding and working with text data and retrieval systems. While there's a substantial amount of information, I will focus on summarizing the key points from each document relevant to the guiding questions.\n",
            "\n",
            "1. **Week12/12_5_Contextual_Text_Mining_Contextual_Probabilistic_Latent_Semantic_Analysis.txt**:\n",
            "   - This document discusses Contextual Probabilistic Latent Semantic Analysis (CPLSA), which incorporates context variables (like time periods or locations) into topic modeling.\n",
            "   - CPLSA aims to discover how topics and their coverage in text change depending on the context.\n",
            "   - The approach involves modeling conditional likelihoods of text given context and the dependency of topics on context, which can be utilized to extract context-specific variations of topics.\n",
            "   - EM algorithm is used for parameter estimation in this model.\n",
            "\n",
            "2. **Week4/4_1_Probabilistic_Retrieval_Model_Basic_Idea.txt**:\n",
            "   - The probabilistic retrieval model is a different approach from the vector space model, and it formulates ranking functions based on the probability of a document being relevant to a query.\n",
            "   - It uses a binary random variable for relevance and deals with random variables instead of vectors.\n",
            "   - Various sub-classes of the probabilistic model, like the language modeling approach, are discussed for effective text retrieval.\n",
            "\n",
            "3. **Week1/1_3_Text_Retrieval_Problem.txt**:\n",
            "   - Text retrieval is the task of a system responding to a user's query with relevant documents.\n",
            "   - The document discusses the difference between text retrieval and database retrieval, highlighting issues like the ambiguity of queries in text retrieval and the structured nature of database queries.\n",
            "   - Two strategies for text retrieval are discussed: document selection (classification) and document ranking. The latter is preferred due to its flexibility and the ability to deal with relative relevancies.\n",
            "\n",
            "4. **Textbook/3Text_Data_Understanding.txt**:\n",
            "   - This chapter from a textbook introduces basic concepts of text data understanding and natural language processing (NLP), including lexical, syntactic, semantic, pragmatic, and discourse analysis.\n",
            "   - Challenges faced in NLP, such as ambiguity and the need for large-scale knowledge representation, are highlighted.\n",
            "   - The utility of statistical language models in text analysis, such as unigram models for simplicity and robustness, is discussed.\n",
            "   - There's an emphasis on how the quality of the text information system is dependent on NLP capability, though some workarounds bypass the need for deep linguistic analysis.\n",
            "\n",
            "To develop answers to the guiding questions, it is crucial to grasp the fundamentals outlined in these documents, especially the concepts of probabilistic models and language models, as well as the challenges and techniques employed in text mining and retrieval systems. Understanding these concepts will enable the viewer to have informed insights into the use of context in text mining, the workings of retrieval models, and the interplay of different NLP tasks in text analysis and understanding.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import concurrent.futures\n",
        "import openai\n",
        "\n",
        "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "def query_gpt4(prompt):\n",
        "    client = openai.OpenAI(api_key=OPENAI_KEY)\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4-1106-preview\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=4096\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def ask_gpt_with_context(question, context_documents, document_names):\n",
        "    prompt = f\"Question: {question}\\n\\nContext:\\n\"\n",
        "    for doc, name in zip(context_documents, document_names):\n",
        "        prompt += f\"Document: {name}\\n{doc}\\n\\n\"\n",
        "\n",
        "    max_length = 128000\n",
        "    if len(prompt) > max_length:\n",
        "        prompt = prompt[-max_length:]\n",
        "\n",
        "    return query_gpt4(prompt)\n",
        "\n",
        "def process_question(query):\n",
        "    try:\n",
        "        top_documents = find_top_documents(query)\n",
        "        top_chapter_result = find_top_documents(query, top_n=1, is_textbook=True)\n",
        "\n",
        "        context_documents = []\n",
        "        context_document_names = []\n",
        "\n",
        "        # Process top lecture transcripts\n",
        "        for doc_name, _ in top_documents:\n",
        "            if doc_name in document_names:\n",
        "                context_documents.append(documents[document_names.index(doc_name)])\n",
        "                context_document_names.append(doc_name)\n",
        "            else:\n",
        "                print(f\"Document '{doc_name}' not found in document_names.\")\n",
        "\n",
        "        # Process top textbook chapter\n",
        "        if top_chapter_result:\n",
        "            chapter_name, _ = top_chapter_result  # Unpack the result\n",
        "            if chapter_name in document_names:\n",
        "                chapter_content = documents[document_names.index(chapter_name)]\n",
        "                context_documents.append(chapter_content)\n",
        "                context_document_names.append(chapter_name)\n",
        "            else:\n",
        "                print(f\"Chapter '{chapter_name}' not found in document_names.\")\n",
        "\n",
        "        answer = ask_gpt_with_context(query, context_documents, context_document_names)\n",
        "\n",
        "        output = {\n",
        "            \"question\": query,\n",
        "            \"documents_used\": context_document_names,\n",
        "            \"answer\": answer\n",
        "        }\n",
        "\n",
        "        return output\n",
        "    except Exception as exc:\n",
        "        print(f\"An error occurred while processing the question '{query}': {exc}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Using ThreadPoolExecutor for parallel processing\n",
        "outputs2 = []\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    # Submit each query (first three questions) to the executor\n",
        "    future_to_query = {executor.submit(process_question, query): query for query in overview_questions[:1]}\n",
        "\n",
        "    # As each future completes, process its result\n",
        "    for future in concurrent.futures.as_completed(future_to_query):\n",
        "        query = future_to_query[future]\n",
        "        try:\n",
        "            output = future.result()\n",
        "            outputs2.append(output)\n",
        "            print(f\"Question: {query}\")\n",
        "            print(f\"Documents used: {', '.join(output['documents_used'])}\")\n",
        "            print(\"Answer:\", output['answer'])\n",
        "            print(\"-------------------------------------------------------------------------------------------------------------\\n\\n\")\n",
        "        except Exception as exc:\n",
        "            print(f\"{query} generated an exception: {exc}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfV-zlo2zKts"
      },
      "source": [
        "# Step 5 (Optional): Generating Guiding Questions and Key Concepts\n",
        "This step addresses the need for creating study aids when guiding questions and key concepts are not readily available. It automates the extraction of essential educational elements from lecture transcripts, ensuring comprehensive support for learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33KeyNc_kMlZ"
      },
      "outputs": [],
      "source": [
        "def process_weekly_lectures(base_directory, weeks_to_process):\n",
        "    all_responses = []\n",
        "\n",
        "    # Function to generate the prompt for GPT\n",
        "    def create_gpt_prompt(week_number, lectures_text):\n",
        "        return (f\"Week {week_number} Lectures for a Master-Level Text Information Systems Course:\\n\\n\" +\n",
        "                \"FORMAT YOUR RESPONSE LIKE\" +\n",
        "                \"Guiding Questions:\\n\" +\n",
        "                \"Q1: [Question 1]\\nA1: [Answer to Question 1]\\n\" +\n",
        "                \"Q2: [Question 2]\\nA2: [Answer to Question 2]\\n\\n\" +\n",
        "                \"Key Concepts:\\n\" +\n",
        "                \"Identify Key Concepts mentioned in the lectures and provide a brief definition for each. Format the response as a numbered list, for example:\\n\" +\n",
        "                \"1. [Term1] - [Definition1]\\n\" +\n",
        "                \"2. [Term2] - [Definition2]\\n\"+\n",
        "                \"______________________________\" +\n",
        "                \"Here are my lecture for the week: \"+\n",
        "                lectures_text +\n",
        "                \"\\nBased on the above lectures, please generate Guiding Questions and Key Concepts as follows:\\n\\n\" +\n",
        "                \"AGAIN, please format the response in this EXACT same format\" +\n",
        "                \"Guiding Questions:\\n\" +\n",
        "                \"Q1: [Question 1]\\nA1: [Answer to Question 1]\\n\" +\n",
        "                \"Q2: [Question 2]\\nA2: [Answer to Question 2]\\n\\n\" +\n",
        "                \"Key Concepts:\\n\" +\n",
        "                \"Identify Key Concepts mentioned in the lectures and provide a brief definition for each. Format the response as a numbered list, for example:\\n\" +\n",
        "                \"1. [Term1] - [Definition1]\\n\" +\n",
        "                \"2. [Term2] - [Definition2]\\n\"+\n",
        "                \"______________________________\"\n",
        "            )\n",
        "\n",
        "    # Iterate over the specified weeks\n",
        "    for week in weeks_to_process:\n",
        "        week_folder = f\"Week{week}\"\n",
        "        week_path = os.path.join(base_directory, week_folder)\n",
        "        if os.path.isdir(week_path):\n",
        "            week_lectures = \"\"\n",
        "            for filename in sorted(os.listdir(week_path)):\n",
        "                file_path = os.path.join(week_path, filename)\n",
        "                if filename.endswith(\".txt\"):\n",
        "                    with open(file_path, 'r') as file:\n",
        "                        week_lectures += file.read() + \"\\n\\n\"\n",
        "\n",
        "            prompt = create_gpt_prompt(week, week_lectures)\n",
        "            response_text = \"Week \" + str(week) + \" Overview: \\n\" + query_gpt4(prompt)\n",
        "            all_responses.append(response_text)\n",
        "\n",
        "    return all_responses\n",
        "\n",
        "# Example usage\n",
        "base_directory = \"/content/drive/MyDrive/CS_410/Text Files/Lectures\"\n",
        "selected_weeks = [1, 2]  # Example: process only weeks 1, 2, and 3\n",
        "responses = process_weekly_lectures(base_directory, selected_weeks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la0zbjZKv6gw",
        "outputId": "0df25405-898a-4283-b83a-b27adf22177e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Guiding Questions:\n",
            "Q: What is Natural Language Processing (NLP) and why is it important in text retrieval?\n",
            "A: Natural Language Processing (NLP) is the main technique for processing natural languages to help computers understand the text data they process. It involves lexical analysis, semantic parsing, and inference among other tasks. It is important in text retrieval because understanding the structure and meaning of text is necessary for effectively finding and organizing relevant information.\n",
            "\n",
            "Q: What are some of the main challenges in natural language processing that affect text retrieval?\n",
            "A: Some main challenges in natural language processing that impact text retrieval include word-level ambiguity, where a word has multiple meanings or syntactic categories (like \"design\" functioning as a noun or a verb); syntactical ambiguities, where a sentence could have multiple interpretations; anaphora resolution, deciding what a pronoun or reference word stands for; and presuppositions, where certain implied information needs to be understood by the system.\n",
            "\n",
            "Q: Why do we need to perform sublinear term frequency (TF) transformation in the vector space model?\n",
            "A: We need to perform sublinear TF transformation to capture the diminishing return of higher term counts in a document and avoid the dominance of a single term over all others. The sublinear transformation, such as the BM25 transformation, helps to ensure that the first occurrence of a term is given more importance than subsequent occurrences, as the initial match is more informative about the document’s relevance to the query.\n",
            "\n",
            "Q: What is document length normalization and why is it important in the vector space model?\n",
            "A: Document length normalization is a process used to penalize long documents that have a higher chance of matching any given query by random chance while avoiding over penalization of documents that are long due to more content as opposed to verbose language. This is important in the vector space model to ensure that document length doesn’t unfairly influence the relevance score of a document. Pivoted length normalization is a common approach where average document length is used as a pivot to adjust the document weight accordingly.\n",
            "\n",
            "Key Concepts:\n",
            "Natural Language Processing (NLP) - A field of study focused on the interaction between computers and human (natural) languages, tasked with enabling computers to understand, interpret, and respond to human language in a valuable way.\n",
            "Lexical Analysis - The process of analyzing text to identify and understand individual words, including their part of speech (nouns, verbs, etc.), often as a first step in the parsing process.\n",
            "Semantic Analysis - A deeper level analysis after syntactical parsing that attempts to deduce the meanings of words, phrases, and sentences in their specific contexts to understand their significance.\n",
            "Bag of Words Representation - A text representation model that ignores the order of words but maintains multiplicity, turning text data into a collection of individual words for analysis purposes.\n",
            "Ranking Function - In text retrieval systems, this function orders documents based on their relevance to a given query, often determined by calculating the similarity between documents' features and the query.\n",
            "Vector Space Model - A model used to represent text documents and queries as vectors of identifiers (like words), where the relevance between documents and queries is calculated based on vector similarity measures such as the cosine similarity.\n",
            "Term Frequency (TF) - A count of how frequently a term appears in a document, often used in scoring the relevance of documents during text retrieval.\n",
            "Document Frequency (DF) - The number of documents in a collection that contain a specific term, used to measure term specificity and weight terms accordingly in relevance calculations.\n",
            "Inference - The logical process of deriving new statements based on the known relationship between existing statements, often part of understanding and reasoning in NLP for text retrieval.\n",
            "Probabilistic Models - Retrieval models that treat queries and documents as observations from random variables and score the relevance as the probability of a document being relevant to a query.\n",
            "Dot Product - A mathematical operation that takes two equal-length sequences of numbers (vectors) and returns a single number, often used to measure vector similarity in vector space models.\n",
            "Query Likelihood Models - A class of probabilistic models in text retrieval where the likelihood of generating the query from a document model is used to score its relevance.\n",
            "Anaphora Resolution - The process of determining which entity a pronoun or a noun phrase refers to within text, which is a challenging aspect of NLP and critical for accurate text interpretation.\n",
            "Term Weight - The relevance or importance of a term in a document or query, which is used in vector space models and other retrieval strategies to score and rank documents.\n",
            "Syntactic Ambiguity - Occurs when a sentence or phrase can be parsed in more than one way, leading to different possible structures and meanings.\n",
            "Vector Space Model (VSM) - A model used to represent text in a multi-dimensional space where each dimension corresponds to a term from the document collection, with the goal of quantifying the relevance of documents to a given query based on the geometric relationship between document and query vectors.\n",
            "Term Frequency (TF) - A count of how many times a term appears in a document, which reflects the importance of the term within that particular document.\n",
            "Inverse Document Frequency (IDF) - A logarithmically scaled ratio used to diminish the weight of terms that appear frequently across multiple documents and boost the weight of terms that are rare in the document collection.\n",
            "BM25 - An advanced ranking function used in text retrieval that incorporates term frequency, inverse document frequency, and document length normalization to calculate the relevance score of a document for a given query.\n",
            "Inverted Index - A data structure used in search engines to store a mapping from content, such as words or phrases, to its locations in a database file, a document, or a set of documents, enabling fast full-text searches.\n",
            "Tokenization - The process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements called tokens to enable further text processing like indexing or search.\n",
            "Document Length Normalization - An adjustment made to a term's significance in a document based on the length of the document, ensuring that longer documents do not have an undue advantage in matching queries.\n",
            "Pivoted Length Normalization - A document length normalization technique that uses the average document length as a pivot point for adjusting a term's weight based on document length.\n",
            "Indexer - A system component that processes text documents to create an index that allows efficient retrieval of documents by content.\n",
            "Scorer - Part of a retrieval system that takes a user's query and the index, calculates the relevance of documents in the collection, and ranks them according to their scores.\n",
            "Feedback Mechanism - A component of a retrieval system that utilizes user interactions, such as clicks and views, to adjust and improve the relevance of search results over time.\n"
          ]
        }
      ],
      "source": [
        "def parse_guiding_questions(responses):\n",
        "    all_parsed_questions = []\n",
        "    question_pattern = re.compile(r\"(?:Q\\d*|[-•]|\\d+\\))\\:? ([^\\n]+)\\n(?:A\\d*|[-•]|\\d+\\))\\:? ([^\\n]+)\")\n",
        "\n",
        "    for response in responses:\n",
        "        questions = question_pattern.findall(response)\n",
        "        for question, answer in questions:\n",
        "            all_parsed_questions.append({'Question': question.strip(), 'Answer': answer.strip()})\n",
        "\n",
        "    return all_parsed_questions\n",
        "\n",
        "def parse_key_concepts(responses):\n",
        "    all_parsed_concepts = []\n",
        "    concept_pattern = re.compile(r\"(\\d+\\.|[-•]) ([^\\-•\\n]+) - ([^\\n]+)\")\n",
        "\n",
        "    for response in responses:\n",
        "        concepts = concept_pattern.findall(response)\n",
        "        for _, term, definition in concepts:\n",
        "            all_parsed_concepts.append({'Term': term.strip(), 'Definition': definition.strip()})\n",
        "\n",
        "    return all_parsed_concepts\n",
        "\n",
        "\n",
        "parsed_questions = parse_guiding_questions(responses)\n",
        "parsed_concepts = parse_key_concepts(responses)\n",
        "\n",
        "# Printing the parsed questions and concepts\n",
        "print(\"Guiding Questions:\")\n",
        "for q in parsed_questions:\n",
        "    print(f\"Q: {q['Question']}\")\n",
        "    print(f\"A: {q['Answer']}\\n\")\n",
        "\n",
        "print(\"Key Concepts:\")\n",
        "for c in parsed_concepts:\n",
        "    print(f\"{c['Term']} - {c['Definition']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsIvYeFg4-MN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "def save_week_overviews(responses, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)  # Create the folder if it doesn't exist\n",
        "\n",
        "    for response in responses:\n",
        "        # Use regular expression to find the week number\n",
        "        match = re.search(r\"Week (\\d+)\", response)\n",
        "        if match:\n",
        "            week_number = match.group(1)\n",
        "        else:\n",
        "            # Fallback to find any number if \"Week X\" format isn't found\n",
        "            match = re.search(r\"\\d+\", response)\n",
        "            week_number = match.group() if match else \"Unknown\"\n",
        "\n",
        "        file_name = f\"Week{week_number}.txt\"\n",
        "        file_path = os.path.join(output_folder, file_name)\n",
        "\n",
        "        # Write the response to the file\n",
        "        with open(file_path, 'w') as file:\n",
        "            file.write(response)\n",
        "\n",
        "\n",
        "output_folder = \"/content/drive/MyDrive/CS_410/Text Files/Generated_Overviews\"  # Replace with your desired output folder path\n",
        "save_week_overviews(responses, output_folder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OQhWHl0lS3h"
      },
      "source": [
        "## Step 6: User Interaction and Query Processing\n",
        "\n",
        "In this step, we focus on the user interface and the core functionality of our educational resource retrieval system. The following functions and processes are implemented to facilitate user interaction:\n",
        "\n",
        "- **Display Menu:** We provide a menu with options for the user to choose from, including getting guiding questions for a specific week, asking a question, or quitting the program.\n",
        "\n",
        "- **Get Guiding Questions:** Users can input a week number, and the system retrieves and displays guiding questions from the corresponding text file. This feature assists learners in accessing relevant course materials.\n",
        "\n",
        "- **Ask a Question:** Users can input their questions, and the system processes these queries by concurrently retrieving documents and querying GPT-4 for answers. The results are presented to the user, including the top documents used and the generated answer.\n",
        "\n",
        "- **Main Program Loop:** The main program loop ensures continuous user interaction, allowing them to navigate through the available options until they choose to exit the program.\n",
        "\n",
        "This step integrates the user interface with the information retrieval and AI-driven question-answering capabilities, making our educational resource system accessible and efficient.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuVZtpdywQr_",
        "outputId": "cf62733b-ca61-4443-c866-f44a98ced83d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Get Guiding Questions\n",
            "2. Ask a Question\n",
            "3. Quit\n",
            "Enter your choice (1-3): 1\n",
            "1. Provided Guiding Questions\n",
            "2. Generated Guiding Questions\n",
            "Choose the type of guiding questions (1-2): 1\n",
            "Available weeks: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n",
            "Enter one of the available week numbers: 3\n",
            "Goals and Objectives\n",
            "After you actively engage in the learning experiences in this module, you should be able to:\n",
            "\n",
            "Explain the Cranfield evaluation methodology and how it works for evaluating a text retrieval system.\n",
            "\n",
            "Explain how to evaluate a set of retrieved documents and how to compute precision, recall, and F1.\n",
            "\n",
            "Explain how to evaluate a ranked list of documents.\n",
            "\n",
            "Explain how to compute and plot a precision-recall curve.\n",
            "\n",
            "Explain how to compute average precision and mean average precision (MAP).\n",
            "\n",
            "Explain how to evaluate a ranked list with multi-level relevance judgments.\n",
            "\n",
            "Explain how to compute normalized discounted cumulative gain.\n",
            "\n",
            "Explain why it is important to perform statistical significance tests.\n",
            "\n",
            "Guiding Questions\n",
            "Develop your answers to the following guiding questions while completing the readings and working on assignments throughout the week.\n",
            "\n",
            "Why is evaluation so critical for research and application development in text retrieval? \n",
            "\n",
            "How does the Cranfield evaluation methodology work? \n",
            "\n",
            "How do we evaluate a set of retrieved documents? \n",
            "\n",
            "How do you compute precision, recall, and F1? \n",
            "\n",
            "How do we evaluate a ranked list of search results? \n",
            "\n",
            "How do you compute average precision? How do you compute mean average precision (MAP) and geometric mean average precision (gMAP)? \n",
            "\n",
            "What is mean reciprocal rank? \n",
            "\n",
            "Why is MAP more appropriate than precision at k documents when comparing two retrieval methods?\n",
            "\n",
            "Why is precision at k documents more meaningful than average precision from a user’s perspective? \n",
            "\n",
            "How can we evaluate a ranked list of search results using multi-level relevance judgments? \n",
            "\n",
            "How do you compute normalized discounted cumulative gain (nDCG)? \n",
            "\n",
            "Why is normalization necessary in nDCG? Does MAP need a similar normalization?  Why is it important to perform statistical significance tests when we compare the retrieval accuracies of two search engine systems?\n",
            "\n",
            "Key Phrases and Concepts\n",
            "Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\n",
            "\n",
            "Cranfield evaluation methodology\n",
            "\n",
            "Precision and recall\n",
            "\n",
            "Average precision, mean average precision (MAP), and geometric mean average precision (gMAP) \n",
            "\n",
            "Reciprocal rank and mean reciprocal rank \n",
            "\n",
            "F-measure \n",
            "\n",
            "Normalized discounted cumulative Gain (nDCG) \n",
            "\n",
            "Statistical significance test  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "1. Get Guiding Questions\n",
            "2. Ask a Question\n",
            "3. Quit\n",
            "Enter your choice (1-3): 2\n",
            "Enter your question: How do you compute precision, recall, and F1? \n",
            "Top Documents: Week3/3_3_Evalution_of_TR_Systems_Evaluating_Ranked_Lists_Part_1.txt, Week3/3_2_Evaluation_of_TR_Systems_Basic_Measures.txt, Textbook/9Search_Engine_Evaluation.txt, Textbook/9Search_Engine_Evaluation.txt\n",
            "Answer: Precision, recall, and F1 score are standard evaluation metrics used to assess the performance of information retrieval systems, classification models, and other predictive algorithms where the output is binary. Here's how to compute them:\n",
            "\n",
            "1. **Precision** (also known as Positive Predictive Value):\n",
            "   This measures the accuracy of the positive predictions. It is defined as the number of true positives divided by the number of true positives plus false positives.\n",
            "   \\[\n",
            "   Precision = \\frac{TP}{TP + FP}\n",
            "   \\]\n",
            "   where \\( TP \\) is the number of true positive results and \\( FP \\) is the number of false positive results.\n",
            "\n",
            "2. **Recall** (also known as Sensitivity or True Positive Rate):\n",
            "   This measures the ability of the model to identify all relevant instances. It is defined as the number of true positives divided by the number of true positives plus false negatives.\n",
            "   \\[\n",
            "   Recall = \\frac{TP}{TP + FN}\n",
            "   \\]\n",
            "   where \\( FN \\) is the number of false negative results.\n",
            "\n",
            "3. **F1 Score**:\n",
            "   The F1 score is the harmonic mean of precision and recall and therefore balances the two metrics. It is useful when you want to seek a balance between precision and recall.\n",
            "   \\[\n",
            "   F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\n",
            "   \\]\n",
            "   or equivalently,\n",
            "   \\[\n",
            "   F1 = \\frac{2TP}{2TP + FP + FN}\n",
            "   \\]\n",
            "   where \\( TP \\), \\( FP \\), and \\( FN \\) are true positives, false positives, and false negatives respectively.\n",
            "\n",
            "The **harmonic mean** is used for combining precision and recall instead of the arithmetic mean because it gives a more significant weight to lower values, which ensures that a good F1 score is obtained only if both precision and recall are high. A high arithmetic mean can be achieved even if one of the two is low and the other is high, which the harmonic mean counteracts. \n",
            "\n",
            "The specific values used to calculate these metrics are usually derived from a confusion matrix, which tabulates the actual vs. predicted classifications in a two-by-two table: true positive, false positive, false negative, and true negative.\n",
            "\n",
            "\n",
            "1. Get Guiding Questions\n",
            "2. Ask a Question\n",
            "3. Quit\n",
            "Enter your choice (1-3): 1\n",
            "1. Provided Guiding Questions\n",
            "2. Generated Guiding Questions\n",
            "Choose the type of guiding questions (1-2): 2\n",
            "Available weeks: 1, 2\n",
            "Enter one of the available week numbers: 2\n",
            "Week 2 Overview: \n",
            "Guiding Questions:\n",
            "Q1: Why do we need to perform sublinear term frequency (TF) transformation in the vector space model?\n",
            "A1: We need to perform sublinear TF transformation to capture the diminishing return of higher term counts in a document and avoid the dominance of a single term over all others. The sublinear transformation, such as the BM25 transformation, helps to ensure that the first occurrence of a term is given more importance than subsequent occurrences, as the initial match is more informative about the document’s relevance to the query.\n",
            "\n",
            "Q2: What is document length normalization and why is it important in the vector space model?\n",
            "A2: Document length normalization is a process used to penalize long documents that have a higher chance of matching any given query by random chance while avoiding over penalization of documents that are long due to more content as opposed to verbose language. This is important in the vector space model to ensure that document length doesn’t unfairly influence the relevance score of a document. Pivoted length normalization is a common approach where average document length is used as a pivot to adjust the document weight accordingly.\n",
            "\n",
            "Key Concepts:\n",
            "1. Vector Space Model (VSM) - A model used to represent text in a multi-dimensional space where each dimension corresponds to a term from the document collection, with the goal of quantifying the relevance of documents to a given query based on the geometric relationship between document and query vectors.\n",
            "   \n",
            "2. Term Frequency (TF) - A count of how many times a term appears in a document, which reflects the importance of the term within that particular document.\n",
            "\n",
            "3. Inverse Document Frequency (IDF) - A logarithmically scaled ratio used to diminish the weight of terms that appear frequently across multiple documents and boost the weight of terms that are rare in the document collection.\n",
            "\n",
            "4. TF-IDF Weighting - A method to evaluate how important a word is to a document in a collection, combining term frequency (TF) and inverse document frequency (IDF).\n",
            "\n",
            "5. BM25 - An advanced ranking function used in text retrieval that incorporates term frequency, inverse document frequency, and document length normalization to calculate the relevance score of a document for a given query.\n",
            "\n",
            "6. Inverted Index - A data structure used in search engines to store a mapping from content, such as words or phrases, to its locations in a database file, a document, or a set of documents, enabling fast full-text searches.\n",
            "\n",
            "7. Tokenization - The process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements called tokens to enable further text processing like indexing or search.\n",
            "\n",
            "8. Document Length Normalization - An adjustment made to a term's significance in a document based on the length of the document, ensuring that longer documents do not have an undue advantage in matching queries.\n",
            "\n",
            "9. Pivoted Length Normalization - A document length normalization technique that uses the average document length as a pivot point for adjusting a term's weight based on document length.\n",
            "\n",
            "10. Indexer - A system component that processes text documents to create an index that allows efficient retrieval of documents by content.\n",
            "\n",
            "11. Scorer - Part of a retrieval system that takes a user's query and the index, calculates the relevance of documents in the collection, and ranks them according to their scores.\n",
            "\n",
            "12. Feedback Mechanism - A component of a retrieval system that utilizes user interactions, such as clicks and views, to adjust and improve the relevance of search results over time.\n",
            "\n",
            "\n",
            "1. Get Guiding Questions\n",
            "2. Ask a Question\n",
            "3. Quit\n",
            "Enter your choice (1-3): Why do we need to perform sublinear term frequency (TF) transformation in the vector space model?\n",
            "Invalid choice. Please enter 1, 2, or 3.\n",
            "1. Get Guiding Questions\n",
            "2. Ask a Question\n",
            "3. Quit\n",
            "Enter your choice (1-3): 3\n",
            "Exiting the program.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import concurrent.futures\n",
        "\n",
        "# Function to display the main menu and get the user's choice\n",
        "def display_menu():\n",
        "    print(\"1. Get Guiding Questions\")\n",
        "    print(\"2. Ask a Question\")\n",
        "    print(\"3. Quit\")\n",
        "    choice = input(\"Enter your choice (1-3): \")\n",
        "    return choice\n",
        "\n",
        "# Function to choose the type of guiding questions (provided or generated)\n",
        "def choose_question_type():\n",
        "    print(\"1. Provided Guiding Questions\")\n",
        "    print(\"2. Generated Guiding Questions\")\n",
        "    choice = input(\"Choose the type of guiding questions (1-2): \")\n",
        "    return choice\n",
        "\n",
        "# Function to get and display guiding questions for a specified week\n",
        "def get_week_options(base_path, subfolder):\n",
        "    \"\"\"Scan the directory to get available week options, sorted numerically.\"\"\"\n",
        "    week_options = []\n",
        "    full_path = os.path.join(base_path, subfolder)\n",
        "    if os.path.exists(full_path):\n",
        "        for filename in os.listdir(full_path):\n",
        "            if filename.startswith(\"Week\") and filename.endswith(\".txt\"):\n",
        "                # Extract the week number from filename and convert to integer\n",
        "                week_number = int(filename[4:-4])\n",
        "                week_options.append(week_number)\n",
        "\n",
        "    # Sort the week numbers in ascending numerical order\n",
        "    week_options = sorted(week_options)\n",
        "\n",
        "    # Convert back to strings\n",
        "    week_options = [str(week) for week in week_options]\n",
        "    return week_options\n",
        "\n",
        "def get_guiding_questions(question_type):\n",
        "    base_path = \"/content/drive/MyDrive/CS_410/Text Files/\"\n",
        "    subfolder = \"Overview\" if question_type == '1' else \"Generated_Overviews\"\n",
        "    week_options = get_week_options(base_path, subfolder)\n",
        "\n",
        "    if week_options:\n",
        "        print(f\"Available weeks: {', '.join(week_options)}\")\n",
        "        week_number = input(\"Enter one of the available week numbers: \")\n",
        "        file_path = os.path.join(base_path, subfolder, f\"Week{week_number}.txt\")\n",
        "\n",
        "        if os.path.exists(file_path):\n",
        "            with open(file_path, 'r') as file:\n",
        "                questions = file.read()\n",
        "                print(questions + \"\\n\\n\")\n",
        "        else:\n",
        "            print(f\"No guiding questions found for week {week_number}.\\n\\n\\n\")\n",
        "    else:\n",
        "        print(f\"No guiding questions available in the {subfolder} folder.\\n\\n\\n\")\n",
        "\n",
        "# Function to ask a question, retrieve documents, and query GPT for an answer\n",
        "def ask_question():\n",
        "    user_question = input(\"Enter your question: \")\n",
        "\n",
        "    # Process the question using concurrent futures\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        future = executor.submit(process_question, user_question)\n",
        "        try:\n",
        "            output = future.result()\n",
        "            print(f\"Top Documents: {', '.join(output['documents_used'])}\")\n",
        "            print(f\"Answer: {output['answer']}\\n\\n\")\n",
        "        except Exception as exc:\n",
        "            print(f\"An error occurred: {exc}\")\n",
        "\n",
        "# Main program function\n",
        "def main():\n",
        "    while True:\n",
        "        choice = display_menu()\n",
        "        if choice == '1':\n",
        "            question_type = choose_question_type()\n",
        "            get_guiding_questions(question_type)\n",
        "        elif choice == '2':\n",
        "            ask_question()\n",
        "        elif choice == '3':\n",
        "            print(\"Exiting the program.\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Invalid choice. Please enter 1, 2, or 3.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQyzJaq1hCAv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5y3W5KeHyVO"
      },
      "source": [
        "# Add On\n",
        "## Create GPT-4 Summary and answer guiding questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImaZrTCgH5CK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import openai\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Assuming the preprocessing functions and other necessary imports and functions are defined as before.\n",
        "\n",
        "# Function to read a file\n",
        "def read_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return file.read()\n",
        "\n",
        "# Function to construct the prompt for GPT-4\n",
        "def construct_prompt(lecture_transcript, overview_content):\n",
        "    prompt = (\n",
        "        \"You are a highly knowledgeable assistant tasked with analyzing a master-level lecture on Cloud Computing Applications. \"\n",
        "        \"Using only the provided lecture transcript and overview document, you are to perform a detailed analysis. \"\n",
        "        \"Remember to reference specific parts of the transcript in your analysis and avoid drawing on outside information. \"\n",
        "        \"Your responses should demonstrate a deep understanding appropriate for a master's program level.\\n\\n\"\n",
        "        \"Lecture Transcript:\\n\"\n",
        "        f\"\\\"\\\"\\\"{lecture_transcript}\\\"\\\"\\\"\\n\\n\"\n",
        "        \"Overview Content (containing Key Terms and Guiding Questions):\\n\"\n",
        "        f\"\\\"\\\"\\\"{overview_content}\\\"\\\"\\\"\\n\\n\"\n",
        "        \"Please provide the following analysis:\\n\"\n",
        "        \"1. Summary: In a clear and concise format, summarize the lecture in a passage of about 300 words. Highlight the main points and crucial ideas, ensuring that your summary is grounded in the specifics of the lecture transcript.\\n\\n\"\n",
        "        \"2. Guiding Questions: For each of the questions listed in the overview document, provide a direct and supported answer. Use quotations from the transcript to substantiate your answers. Detail your reasoning process for each response to demonstrate how you arrived at your conclusion.\\n\\n\"\n",
        "        \"3. Key Terms: Identify and explain the significance of each key term listed in the overview. Discuss how these terms are applied in the context of the lecture transcript, providing specific examples or explanations from the transcript.\\n\\n\"\n",
        "        \"Aim to format your analysis with clear, distinct sections for each part, using bullet points or numbered lists where appropriate. Your analysis is crucial for understanding and reviewing the core insights of the lecture. Thank you for your detailed work.\"\n",
        "    )\n",
        "    return prompt\n",
        "\n",
        "# Function to save the output to a text file\n",
        "def save_output(file_path, content):\n",
        "    with open(file_path, 'w') as file:\n",
        "        file.write(content)\n",
        "\n",
        "# Function to process all files for a single week and create one final analysis with GPT-4\n",
        "def process_week_final_prompt(week_num, base_directory):\n",
        "    week_directory = f\"{base_directory}/Week{week_num}\"\n",
        "    overview_directory = f\"{week_directory}/Overview\"\n",
        "    transcripts_directory = f\"{week_directory}/Transcripts\"\n",
        "\n",
        "    # Read the overview file\n",
        "    overview_file_path = f\"{overview_directory}/Week{week_num}_Overview.txt\"\n",
        "    overview_content = read_file(overview_file_path)\n",
        "\n",
        "    # Initialize a variable to store all lecture transcripts for the week\n",
        "    all_lectures_transcript = \"\"\n",
        "\n",
        "    # Get all lecture video files for the week\n",
        "    lecture_files = [f for f in os.listdir(transcripts_directory) if f.endswith('.txt')]\n",
        "\n",
        "    # Concatenate each lecture transcript\n",
        "    for lecture_file in lecture_files:\n",
        "        lecture_file_path = f\"{transcripts_directory}/{lecture_file}\"\n",
        "        lecture_transcript = read_file(lecture_file_path)\n",
        "        all_lectures_transcript += f\"{lecture_transcript}\\n\\n\"\n",
        "\n",
        "    # Construct the final prompt for GPT-4\n",
        "    final_prompt = construct_prompt(all_lectures_transcript, overview_content)\n",
        "\n",
        "    # Query GPT-4 with the final prompt\n",
        "    final_gpt_response = query_gpt4(final_prompt)\n",
        "\n",
        "    # Save the final GPT-4 response to a text file for the whole week\n",
        "    final_output_file_path = f\"{week_directory}/Week{week_num}_Final_GPT4_Analysis.txt\"\n",
        "    save_output(final_output_file_path, final_gpt_response)\n",
        "\n",
        "# Example usage:\n",
        "process_week_final_prompt(1, \"/content/drive/MyDrive/UIUC/CS_498/Week1\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
